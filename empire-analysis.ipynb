{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "# Analysis of the State and Evolution of Empirical Research in Requirements Engineering\n",
    "\n",
    "**Remark:** This Jupyter Notebook is supplementary material for the paper \"*Divide and Conquer the EmpiRE: A Community-Maintained Body of Knowledge of Empirical Research in Requirements Engineering*\", submitted to the [17th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement](https://conf.researchr.org/home/esem-2023).\n",
    "\n",
    "<details>\n",
    "  <summary>Table of Contents</summary>\n",
    "\n",
    "1. [Summary](#step1)\n",
    "2. [Reusable Functions for Data Analysis](#step2)\n",
    "3. [Analysis of Competency Question](#step3)\n",
    "    \n",
    "    3.1 [How has the number of empirical studies evolved over time?](#q1)\n",
    "\n",
    "    3.2 [How often are which empirical methods used over time?](#q2)\n",
    "    \n",
    "    3.3 [How has the number of papers that do not have an empirical study evolved over time?](#q3)\n",
    "    \n",
    "    3.4 [How often are which empirical methods used?](#q4)\n",
    "\n",
    "    3.5 [How have the proportions of experiments, secondary research (reviews), surveys, case studies, and action research in the empirical methods used evolved over time?](#q5)\n",
    "    \n",
    "    3.6 [How often are which statistical methods used?](#q6)\n",
    "    \n",
    "    3.7 [How has the use of statistical methods evolved over time?](#q7)\n",
    "    \n",
    "    3.8 [How has the reporting of threats to validity evolved over time?](#q8)\n",
    "    \n",
    "    3.9 [What types of threats to validity do the authors report?](#q9)\n",
    "    \n",
    "    3.10 [How have the proportions of case studies and action research in the empirical methods used evolved over time?](#q10)\n",
    "    \n",
    "    3.11 [How has the provision of data (the materials used, the raw data collected, and the study results identified) evolved over time?](#q11)\n",
    "    \n",
    "    3.12 [How has the reporting of research questions and answers evolved over time?](#q12)\n",
    "    \n",
    "    3.13 [What empirical methods are used to conduct integrative and interpretive (systematic literature) reviews, so-called secondary research?](#q13)\n",
    "    \n",
    "    3.14 [How has the proportions of empirical methods to conduct (systematic literature) reviews, so-called secondary research, evolved over time?](#q14)\n",
    "    \n",
    "    3.15 [How many different empirical methods are used per publication?](#q15)\n",
    "    \n",
    "    3.16 [How has the number of empirical methods used per publication evolved over time?](#q16)\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## 1. Summary\n",
    "\n",
    "## Context\n",
    "Empirical research in Requirements Engineering (RE) has become an important topic and research area. Over the years, several researchers examined how empirical research in RE is \"currently\" conducted and how it should be conducted in the future by presenting snapshots of the \"current\" state of empirical research in RE and, more generally, in Software Engineering (SE). These researchers share the same goal of synthesizing a comprehensive, current, and long-term available overview of the state and evolution of empirical research in RE and SE. Although they share the same goal, use similar methods, i.e., (systematic) literature reviews or systematic mapping studies, and even examine overlapping periods, venues, and themes, they have not collaborated to build on and update previous work. **Lack of collaboration and updating are known challenges of literature reviews.** Overcoming these challenges is critical to ensure the quality, reliability, and timeliness of research findings from literature reviews.\n",
    "\n",
    "## Motivation\n",
    "Current research addresses the above challenges by focusing on when and how to update (systematic) literature reviews in SE and its subfields. While these works provide social and economic decision support and guidance for updating literature reviews, **the central problem is the unavailability of the extracted and analyzed data**, corresponding to open science in SE. Unavailable data complicates collaboration among researchers in updating a literature review as the entire data collection, extraction, and analysis must be repeated and expanded for a comprehensive review. Researchers need adequate technical support in the form of infrastructures and services to achieve sustainable literature reviews with all available data. These infrastructures must ensure that the data is Findable, Accessible, Interoperable, and Reusable (FAIR) over the long term following the FAIR data principles. For this purpose, the data must be organized in a flexible, fine-grained, context-sensitive, and semantic representation to be understandable, processable, and useful to humans and machines. Over the last decade, Knowledge Graphs (KGs) became an emerging technology in industry and academia as they enable this versatile data representation. Besides well-known KGs for encyclopedic and factual data, such as [DBpedia](https://www.dbpedia.org/) and [WikiData](https://www.wikidata.org), using so-called Research Knowledge Graphs (RKGs) for scientific data is a rather new approach. RKGs include bibliographic metadata, e.g., titles, authors, and venues, as well as scientific data, e.g., research designs, methods, and results. They are a promising technology to sustainably organize scientific data, e.g., extracted and analyzed data of literature reviews, as a body of knowledge that can be built, published, maintained, (re)used, updated, and expanded in a long-term and collaborative manner.\n",
    "\n",
    "## Objective\n",
    "**Our longterm objective is to develop a body of knowledge that synthesizes a comprehensive, current, and long-term available overview of the state and evolution of empirical research in RE**. For this purpose, we start to explore using RKGs as technical infrastructure for building, publishing, (re)using, updating, and expanding an initial body of knowledge of empirical research in RE (EmpiRE), that the research community can maintain by *dividing* the efforts to *conquer* the EmpiRE. In particular, we explore using the Open Research Knowledge Graph ([ORKG](https://orkg.org/)), a cross-domain and cross-topic RKG that provides accompanying services using intertwined human and machine intelligence by combining manual crowdsourcing and automated approaches to organize scientific data. With this work, we lay the foundation for such a comprehensive, current, and long-term available overview of the state and evolution of empirical research in RE by building, publishing, and evaluating an initial informed and reflected body of knowledge of empirical research in RE.\n",
    "\n",
    "## Approach\n",
    "<p align=\"center\">\n",
    "    <img src=\"Figures/approach.png\" width=\"600\"/>\n",
    "    \n",
    "</p>\n",
    "<p align=\"center\">\n",
    "    <em>Research approach for developing, publishing, and evaluating an intial informed and reflected body of knowledge of empirical research in RE.</em>\n",
    "</p>\n",
    "\n",
    "Our research approach consists of three steps: Data collection, Data extraction, and Data analysis. So far, **we collected 411 papers** published in the research track of the [IEEE International Requirements Engineering Conference](https://ieeexplore.ieee.org/xpl/conhome/1000630/all-proceedings) from 2006 to 2022. **We extracted and organized their scientific data**, i.a., research paradigm, research design, empirical method (data collection and data analysis), and bibliographic metadata using a developed [ORKG template](https://orkg.org/template/R186491). ORKG templates allow specifying the structure of descriptions of papers similar to SHACL shapes. In this way, we determined which data should be extracted and ensured that all the descriptions of papers are consistent and comparable to **build and publish an initial informed and reflected body of knowledge of empirical research in RE**.\n",
    "\n",
    "In this Jupyter Notebook, we perform the data analysis of the developed body of knowledge of empirical research in RE, which has two purposes:\n",
    "\n",
    "(1) We want to get first insights into the state and evolution of empirical research in RE by analyzing the developed body of knowledge.\n",
    "\n",
    "(2) We want to evaluate the body of knowledge concerning its coverage of the curated topic.\n",
    "\n",
    "The data analysis is based on competency questions regarding empirical research in all fields of SE, including RE, derived from the vision of [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30). [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30) describe their vision of the role of empirical methods in all fields of SE, including RE, for the current period of 2020 – 2025 by comparing the **\"current\" state of practice (2007)** with their **target state (2020 - 2025)**. We analyzed these descriptions and derived a total of [77 competency questions](competency-questions.xlsx). For each competency question, we assessed whether we can answer the question with the developed body of knowledge. Subsequently, we specified the answerable competency questions as [SPARQL](https://www.w3.org/TR/sparql11-query/) queries to retrieve and analyze the data from the [ORKG](https://orkg.org/). In this way, **we obtain answers to the competency questions and thus gain first insights into the state and evolution of empirical research in RE**.\n",
    "\n",
    "Below, we present an overview of the competency question answered. The details of each analysis are presented in corresponding sections.\n",
    "\n",
    "<details>\n",
    "  <summary>Competency questions answered:</summary>\n",
    "  \n",
    "1. [How has the number of empirical studies evolved over time?](#q1)\n",
    "2. [How often are which empirical methods used over time?](#q2)\n",
    "3. [How has the number of papers that do not have an empirical study evolved over time?](#q3)\n",
    "4. [How often are which empirical methods used?](#q4)\n",
    "5. [How have the proportions of experiments, secondary research (reviews), surveys, case studies, and action research in the empirical methods used evolved over time?](#q5)\n",
    "6. [How often are which statistical methods used?](#q6)\n",
    "7. [How has the use of statistical methods evolved over time?](#q7)\n",
    "8. [How has the reporting of threats to validity evolved over time?](#q8)\n",
    "9. [What types of threats to validity do the authors report?](#q9)\n",
    "10. [How have the proportions of case studies and action research in the empirical methods used evolved over time?](#q10)\n",
    "11. [How has the provision of data (the materials used, the raw data collected, and the study results identified) evolved over time?](#q11)\n",
    "12. [How has the reporting of research questions and answers evolved over time?](#q12)\n",
    "13. [What empirical methods are used to conduct integrative and interpretive (systematic literature) reviews, so-called secondary research?](#q13)\n",
    "14. [How has the proportions of empirical methods to conduct (systematic literature) reviews, so-called secondary research, evolved over time?](#q14)\n",
    "15. [How many different empirical methods are used per publication?](#q15)\n",
    "16. [How has the number of empirical methods used per publication evolved over time?](#q16)\n",
    "</details>\n",
    "\n",
    "## Results\n",
    "At the moment, we can answer 16 of the 77 competency questions with the developed body of knowledge.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## 2. Reusable Functions for Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from SPARQLWrapper import SPARQLWrapper, CSV\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_area_auto_adjustable\n",
    "import dataframe_image as dfi\n",
    "\n",
    "ENDPOINT_URL = \"https://www.orkg.org/triplestore\"\n",
    "\n",
    "PREFIXES =  \"\"\"\n",
    "            PREFIX orkgr: <http://orkg.org/orkg/resource/>\n",
    "            PREFIX orkgc: <http://orkg.org/orkg/class/>\n",
    "            PREFIX orkgp: <http://orkg.org/orkg/predicate/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "            PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "            PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "            \"\"\"\n",
    "\n",
    "DATE = '2023-03-30'\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 8)\n",
    "sns.set_context('talk')\n",
    "sns.set_style('whitegrid')\n",
    "sns.set(font_scale=1.25)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def retrieve_data(id, query):\n",
    "        \n",
    "        sparql = SPARQLWrapper(ENDPOINT_URL)#, agent=user_agent)\n",
    "        sparql.setQuery(PREFIXES+query)\n",
    "        sparql.setReturnFormat(CSV)\n",
    "        \n",
    "        try:\n",
    "                results = sparql.queryAndConvert()\n",
    "        except Exception as e:\n",
    "                print(e)\n",
    "        \n",
    "        now = datetime.now()\n",
    "        with open('Data/query_' + id + '_data_' + now.strftime('%Y-%m-%d') + '.csv', 'wb') as file:\n",
    "                file.write(results)\n",
    "\n",
    "def explore_data(df):\n",
    "        display('Number of total entries in the dataset: ' + str(df.shape[0]))\n",
    "        display('Number of unique entries in the dataset: ' + str(df.index.nunique()))\n",
    "        display(df.head(15))\n",
    "        display(df.info())\n",
    "\n",
    "        #Missing value analysis\n",
    "        missing_values = df.isna().sum()\n",
    "        plt.figure()\n",
    "        ax_exp = missing_values.plot(kind='barh')\n",
    "        ax_exp.set_xlim(left=0)\n",
    "        #start, end = ax_exp.get_xlim()\n",
    "        #ax_exp.xaxis.set_ticks(np.arange(start, end, 1))\n",
    "        plt.title('Number of missing values per column in the dataset')\n",
    "        plt.xlabel('Number of missing values')\n",
    "        plt.ylabel('Column of the dataset')\n",
    "        plt.show()\n",
    "\n",
    "        # Proportion of missing values in the entire data set \n",
    "        data = missing_values / df.shape[0] * 100\n",
    "        display(pd.DataFrame(data, columns=[\"Proportion of missing values\"]).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## 3. Analysis of the Competency Questions\n",
    "\n",
    "Below, we present the individual analyses for each answerable competency question. Each analysis follows the same structure:\n",
    "\n",
    "1. Data Selection: Explaining the competency question and the required data for the analysis.\n",
    "2. Data Collection: Executing the specified SPARQL query.\n",
    "3. Data Exploration: Exploring the data, including its cleaning and validation, to prepare the data for data analysis.\n",
    "4. Data Analysis: Analyzing the data and creating visualizations.\n",
    "5. Data Interpretation: Interpreting the data and derive insights.\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q1'></a>\n",
    "### 3.1 How has the number of empirical studies evolved over time?\n",
    "\n",
    "#### <ins>*Data Selection*</ins> \n",
    "\n",
    "*Explanation of the Competency Question*:\n",
    "\n",
    "According to [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30), the **\"current\" state of practice (2007)** shows that there are relatively **few empirical studies**. For the **target state (2020 - 2025)**, [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30) envision a **large number of studies**. This predicted change from a few to a large number of empirical studies leads to the corresponding competency question.\n",
    "\n",
    "*Required Data for the Analysis*:\n",
    "\n",
    "We must retrieve all papers with their publication year that use our ORKG template and report an empirical study. However, we need to define what we mean by an empirical study. According to [Empirical Software Engineering Journal](https://www.springer.com/journal/10664), \"*Empirical studies presented here usually involve the collection and analysis of data and experience...*\". For this reason, we define that an empirical study is a study that includes data analysis as a necessary condition to be a study (*Necessity*) and data collection as a sufficient condition to be an empirical study (*Sufficiency*). Thus, a study must always include data analysis and an empirical study must include data collection and data analysis. We do not consider the mere reporting of a data collection as a study or even an empirical study.\n",
    "\n",
    "[Back to top](#step3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Collection*</ins> \n",
    "Below, we retrieve the required data from the developed body of knowledge of empirical research in RE from the [ORKG](https://www.orkg.org/orkg/) using its [SPARQL endpoint](https://orkg.org/sparql/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = '1'\n",
    "\n",
    "query_1 = \"\"\"\n",
    "        SELECT ?paper, ?year, ?dc_label, ?da_label\n",
    "        WHERE {\n",
    "                ?paper orkgp:P31 ?contribution;\n",
    "                        orkgp:P29 ?year.\n",
    "                ?contribution a orkgc:C27001.\n",
    "                \n",
    "                OPTIONAL{?contribution orkgp:P56008 ?data_collection.\n",
    "                        ?data_collection rdfs:label ?dc_label.\n",
    "                }\n",
    "                OPTIONAL{?contribution orkgp:P15124 ?data_analysis.\n",
    "                        ?data_analysis rdfs:label ?da_label.\n",
    "                }\n",
    "                FILTER(xsd:integer(?year) > \"2005\"^^xsd:integer)\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_1)\n",
    "\n",
    "df_query_1 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Exploration*</ins>\n",
    "Below, we explore the retrieved data, including its cleaning and validation, to prepare the data for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_1['dc_label'] = df_query_1['dc_label'].astype('category')\n",
    "df_query_1['da_label'] = df_query_1['da_label'].astype('category')\n",
    "display(df_query_1['dc_label'].value_counts())\n",
    "display(df_query_1['da_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Analysis*</ins>\n",
    "For this data analysis, we select all papers that have an empirical study according to our definition (data collection and data analysis). For this reason, we remove all papers that have \"no collection\" and/or \"no analysis\". In addition, a paper can involve more than one empirical method for data collection and data analysis so that we must exclude duplicate papers. In this way, we can determine the number of all unique papers. For more detailed insights, we normalize the number of all papers with an empirical study based on the number of all unique papers per year, as the total number of papers per year varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(df_query_1.drop_duplicates(subset=['paper']).reset_index(drop=True)['year'].value_counts())\n",
    "result.columns = ['number_of_all_papers']\n",
    "\n",
    "papers_with_emp_study = df_query_1[(df_query_1['dc_label'] != 'no collection') & (df_query_1['da_label'] != 'no analysis')]\n",
    "papers_with_emp_study = papers_with_emp_study.drop_duplicates(subset=['paper'])\n",
    "result['number_of_papers_with_emp_studies'] = papers_with_emp_study.reset_index(drop=True)['year'].value_counts()\n",
    "\n",
    "result['normalized_papers_with_emp_studies'] = (result['number_of_papers_with_emp_studies'] / result['number_of_all_papers']).round(2)\n",
    "#display(result.sort_index())\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.barplot(data=result, x=result.index, y='number_of_papers_with_emp_studies', color='b')\n",
    "ax.bar_label(ax.containers[0])\n",
    "make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Number of papers with an empirical study per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of papers with an empirical study')\n",
    "plt.savefig('Figures/CQ1/number_of_papers_with_empirical_study.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.barplot(data=result, x=result.index, y='normalized_papers_with_emp_studies', color='b')\n",
    "ax.bar_label(ax.containers[0], fmt='%.2f')\n",
    "make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Normalized number of papers with an empirical study per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Proportion of papers with an empirical study')\n",
    "plt.savefig('Figures/CQ1/proportion_of_papers_with_empirical_study.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Interpretation*</ins>\n",
    "Muss noch geschrieben werden, wenn die Daten alle geprüft sind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q2'></a>\n",
    "### 3.2 How often are which empirical methods used over time?\n",
    "\n",
    "#### <ins>*Data Selection*</ins> \n",
    "\n",
    "*Explanation of the Competency Question*:\n",
    "\n",
    "According to [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30), the **\"current\" state of practice (2007)** shows a that there are relatively **few empirical studies**. For the **target state (2020 - 2025)**, [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30) envision a **large number of studies [...] using different empirical methods**. This predicted change from a few to a large number of empirical studies using different empirical methods leads to the corresponding competency question.\n",
    "\n",
    "*Required Data for the Analysis*:\n",
    "\n",
    "We must retrieve all papers with their publication year that use our ORKG template and report on the use of empirical methods. According to [Dan (2017)](https://doi.org/10.1002/9781118901731.iecrm0083), empirical methods include data collection and data analysis. An empirical method can therefore be a method for data collection and data analysis. We consider the empirical method used for data collection or data analysis respectively, as our template allows for a correspondingly more fine-grained analysis.\n",
    "\n",
    "[Back to top](#step3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Collection*</ins> \n",
    "Below, we retrieve the required data from the developed body of knowledge of empirical research in RE from the [ORKG](https://www.orkg.org/orkg/) using its [SPARQL endpoint](https://orkg.org/sparql/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = '2.1'\n",
    "\n",
    "query_2_1_data_collection = \"\"\"\n",
    "        SELECT ?paper, ?year, ?dc_label\n",
    "        WHERE {\n",
    "                ?paper orkgp:P31 ?contribution;\n",
    "                        orkgp:P29 ?year.\n",
    "                ?contribution a orkgc:C27001.\n",
    "\n",
    "                OPTIONAL{?contribution orkgp:P56008 ?data_collection.\n",
    "                        ?data_collection rdfs:label ?dc_label.\n",
    "                }\n",
    "                FILTER(?dc_label != \"no collection\"^^xsd:string)\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_2_1_data_collection)\n",
    "df_query_2_1 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')\n",
    "\n",
    "ID = '2.2'\n",
    "query_2_2_data_analysis = \"\"\"\n",
    "        SELECT ?paper, ?year, ?da_label, ?descriptive, ?inferential, ?machine_learning, ?method\n",
    "        WHERE {\n",
    "                ?paper orkgp:P31 ?contribution;\n",
    "                        orkgp:P29 ?year.\n",
    "                ?contribution a orkgc:C27001.\n",
    "                \n",
    "                \n",
    "                OPTIONAL{?contribution orkgp:P15124 ?data_analysis.\n",
    "                        ?data_analysis rdfs:label ?da_label.\n",
    "                        \n",
    "                        OPTIONAL{?data_analysis orkgp:P56048/rdfs:label ?descriptive.}\n",
    "                        OPTIONAL{?data_analysis orkgp:P56043/rdfs:label ?inferential.}\n",
    "                        OPTIONAL{?data_analysis orkgp:P57016/rdfs:label ?machine_learning.}\n",
    "                        OPTIONAL{?data_analysis orkgp:P76003/rdfs:label ?method}\n",
    "                }\n",
    "                FILTER(?da_label != \"no analysis\"^^xsd:string)\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_2_2_data_analysis)\n",
    "df_query_2_2 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Exploration*</ins>\n",
    "Below, we explore the retrieved data, including its cleaning and validation, to prepare the data for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_2_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_2_1['dc_label'] = df_query_2_1['dc_label'].astype('category')\n",
    "display(df_query_2_1['dc_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_2_2['da_label'] = df_query_2_2['da_label'].astype('category')\n",
    "df_query_2_2['descriptive'] = df_query_2_2['descriptive'].astype('category')\n",
    "df_query_2_2['inferential'] = df_query_2_2['inferential'].astype('category')\n",
    "df_query_2_2['machine_learning'] = df_query_2_2['machine_learning'].astype('category')\n",
    "df_query_2_2['method'] = df_query_2_2['method'].astype('category')\n",
    "\n",
    "display(df_query_2_2['da_label'].value_counts())\n",
    "display(df_query_2_2['descriptive'].value_counts())\n",
    "display(df_query_2_2['inferential'].value_counts())\n",
    "display(df_query_2_2['machine_learning'].value_counts())\n",
    "display(df_query_2_2['method'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Analysis*</ins>\n",
    "For this data analysis, we consider the empirical methods used for data collection. We identify the empirical methods used for data collection. A paper can involve more than one empirical method for data collection so that the number of empirical methods can be larger than the number of papers. In addition, the number of papers per year varies. For this reason, we normalize the number of empirical methods used based on the number of all unique papers per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers_per_year = pd.DataFrame(df_query_2_1.drop_duplicates(subset=['paper']).reset_index(drop=True)['year'].value_counts())\n",
    "all_papers_per_year.columns = ['number_of_papers_with_dc']\n",
    "all_papers_per_year = all_papers_per_year.sort_index()\n",
    "\n",
    "result = pd.DataFrame(df_query_2_1.groupby('year')['dc_label'].value_counts().unstack())\n",
    "result = result[result.sum().sort_values(ascending=False).index]\n",
    "result = result.reindex(columns = [col for col in result.columns if col != 'study'] + ['study'])\n",
    "display(result)\n",
    "\n",
    "ax = result.plot(kind='bar', rot=0)\n",
    "ax.legend(bbox_to_anchor=(0.9, 0.77), labels=['Experiment', 'Case study', 'Secondary research', 'Interview', 'Survey', 'Action research', 'Other method'])\n",
    "make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Number of empirical methods used for data collection per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of empirical method used')\n",
    "plt.savefig('Figures/CQ2/dc_number_of_empirical_methods_used.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "figure, axis = plt.subplots(4, 2, figsize=(18, 15), sharey=True)\n",
    "# Plot barplots\n",
    "bar1 = sns.barplot(data = result, x = result.index, y='experiment', ax=axis[0,0], color=sns.color_palette()[0])\n",
    "bar1.bar_label(bar1.containers[0])\n",
    "make_axes_area_auto_adjustable(bar1)\n",
    "bar2 = sns.barplot(data = result, x = result.index, y='case study', ax=axis[0,1], color=sns.color_palette()[1])\n",
    "bar2.bar_label(bar2.containers[0])\n",
    "make_axes_area_auto_adjustable(bar2)\n",
    "bar3 = sns.barplot(data = result, x = result.index, y='secondary research', ax=axis[1,0], color=sns.color_palette()[2])\n",
    "bar3.bar_label(bar3.containers[0])\n",
    "make_axes_area_auto_adjustable(bar3)\n",
    "bar4 = sns.barplot(data = result, x = result.index, y='interview', ax=axis[1,1], color=sns.color_palette()[3])\n",
    "bar4.bar_label(bar4.containers[0])\n",
    "make_axes_area_auto_adjustable(bar4)\n",
    "bar5 = sns.barplot(data = result, x = result.index, y='survey', ax=axis[2,0], color=sns.color_palette()[4])\n",
    "bar5.bar_label(bar5.containers[0])\n",
    "make_axes_area_auto_adjustable(bar5)\n",
    "bar6 = sns.barplot(data = result, x = result.index, y='action research', ax=axis[2,1], color=sns.color_palette()[5])\n",
    "bar6.bar_label(bar6.containers[0])\n",
    "make_axes_area_auto_adjustable(bar6)\n",
    "bar7 = sns.barplot(data = result, x = result.index, y='study', ax=axis[3,0], color=sns.color_palette()[6])\n",
    "bar7.bar_label(bar7.containers[0])\n",
    "make_axes_area_auto_adjustable(bar7)\n",
    "\n",
    "# Remove unnecessary subplot\n",
    "figure.delaxes(axis[3,1])\n",
    "\n",
    "# Set titles\n",
    "plt.suptitle(\"Number of empirical method used for data collection per year grouped by empirical method\")\n",
    "bar1.set(title = 'Number of experiments used for data collection per year', ylabel='Number of experiments', xlabel='Year')\n",
    "bar2.set(title = 'Number of case studies used for data collection per year', ylabel='Number of case studies', xlabel='Year')\n",
    "bar3.set(title = 'Number of secondary research used for data collection per year', ylabel='Number of secondary research', xlabel='Year')\n",
    "bar4.set(title = 'Number of interviews used for data collection per year', ylabel='Number of interviews', xlabel='Year')\n",
    "bar5.set(title = 'Number of surveys used for data collection per year', ylabel='Number of surveys', xlabel='Year')\n",
    "bar6.set(title = 'Number of actions research used for data collection per year', ylabel='Number of action research', xlabel='Year')\n",
    "bar7.set(title = 'Number of other methods used for data collection per year', ylabel='Number of other methods', xlabel='Year')\n",
    "\n",
    "# Set spacing between subplots\n",
    "figure.tight_layout()\n",
    "plt.savefig('Figures/CQ2/dc_number_of_empirical_methods_used_grouped_by_method.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "result = pd.concat([result, all_papers_per_year], axis=1)\n",
    "result['normalized experiment'] = (result['experiment'] / result['number_of_papers_with_dc']).round(2)\n",
    "result['normalized case study'] = (result['case study'] / result['number_of_papers_with_dc']).round(2)\n",
    "result['normalized secondary research'] = (result['secondary research'] / result['number_of_papers_with_dc']).round(2)\n",
    "result['normalized interview'] = (result['interview'] / result['number_of_papers_with_dc']).round(2)\n",
    "result['normalized survey'] = (result['survey'] / result['number_of_papers_with_dc']).round(2)\n",
    "result['normalized action research'] = (result['action research'] / result['number_of_papers_with_dc']).round(2)\n",
    "result['normalized study'] = (result['study'] / result['number_of_papers_with_dc']).round(2)\n",
    "#display(result)\n",
    "\n",
    "ax = result.loc[:, 'normalized experiment':'normalized study'].plot(kind='bar', rot=0)\n",
    "ax.legend(bbox_to_anchor=(0.4, 0.65), labels=['Experiment', 'Case study', 'Secondary research', 'Interview', 'Survey', 'Action research', 'Other method'])\n",
    "make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Normalized number of empirical methods used for data collection per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Proportion of empirical method used')\n",
    "plt.savefig('Figures/CQ2/dc_proportion_of_empirical_methods_used.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "figure, axis = plt.subplots(4, 2, figsize=(18, 15), sharey=True)\n",
    "# Plot barplots\n",
    "bar1 = sns.barplot(data = result, x = result.index, y='normalized experiment', ax=axis[0,0], color=sns.color_palette()[0])\n",
    "bar1.bar_label(bar1.containers[0])\n",
    "make_axes_area_auto_adjustable(bar1)\n",
    "bar2 = sns.barplot(data = result, x = result.index, y='normalized case study', ax=axis[0,1], color=sns.color_palette()[1])\n",
    "bar2.bar_label(bar2.containers[0])\n",
    "make_axes_area_auto_adjustable(bar2)\n",
    "bar3 = sns.barplot(data = result, x = result.index, y='normalized secondary research', ax=axis[1,0], color=sns.color_palette()[2])\n",
    "bar3.bar_label(bar3.containers[0])\n",
    "make_axes_area_auto_adjustable(bar3)\n",
    "bar4 = sns.barplot(data = result, x = result.index, y='normalized interview', ax=axis[1,1], color=sns.color_palette()[3])\n",
    "bar4.bar_label(bar4.containers[0])\n",
    "make_axes_area_auto_adjustable(bar4)\n",
    "bar5 = sns.barplot(data = result, x = result.index, y='normalized survey', ax=axis[2,0], color=sns.color_palette()[4])\n",
    "bar5.bar_label(bar5.containers[0])\n",
    "make_axes_area_auto_adjustable(bar5)\n",
    "bar6 = sns.barplot(data = result, x = result.index, y='normalized action research', ax=axis[2,1], color=sns.color_palette()[5])\n",
    "bar6.bar_label(bar6.containers[0])\n",
    "make_axes_area_auto_adjustable(bar6)\n",
    "bar7 = sns.barplot(data = result, x = result.index, y='normalized study', ax=axis[3,0], color=sns.color_palette()[6])\n",
    "bar7.bar_label(bar7.containers[0])\n",
    "make_axes_area_auto_adjustable(bar7)\n",
    "\n",
    "# Remove unnecessary subplot\n",
    "figure.delaxes(axis[3,1])\n",
    "\n",
    "# Set titles\n",
    "plt.suptitle(\"Normalized number of empirical methods used for data collection per year grouped by empirical method\")\n",
    "bar1.set(title = 'Normalized number of experiments used for data collection per year', ylabel='Proportion of experiments', xlabel='Year')\n",
    "bar2.set(title = 'Normalized number of case studies used for data collection per year', ylabel='Proportion of case studies', xlabel='Year')\n",
    "bar3.set(title = 'Normalized number of secondary research used for data collection per year', ylabel='Proportion of secondary research', xlabel='Year')\n",
    "bar4.set(title = 'Normalized number of interviews used for data collection per year', ylabel='Proportion of interviews', xlabel='Year')\n",
    "bar5.set(title = 'Normalized number of surveys used for data collection per year', ylabel='Proportion of surveys', xlabel='Year')\n",
    "bar6.set(title = 'Normalized number of actions research used for data collection per year', ylabel='Proportion of action research', xlabel='Year')\n",
    "bar7.set(title = 'Normalized number of other methods used for data collection per year', ylabel='Proportion of other methods', xlabel='Year')\n",
    "\n",
    "# Set spacing between subplots\n",
    "figure.tight_layout()\n",
    "plt.savefig('Figures/CQ2/dc_proportion_of_empirical_methods_used_grouped_by_method.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data analysis, we consider the empirical methods used for data analysis. We identify the empirical methods used for data analysis. A paper can involve more than one empirical method for data analysis so that the number of empirical methods can be larger than the number of papers. In addition, the number of papers per year varies. For this reason, we normalize the number of empirical methods used based on the number of all unique papers per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers_per_year = pd.DataFrame(df_query_2_2.drop_duplicates(subset=['paper']).reset_index(drop=True)['year'].value_counts())\n",
    "all_papers_per_year.columns = ['number_of_papers_with_da']\n",
    "all_papers_per_year = all_papers_per_year.sort_index()\n",
    "#display(all_papers_per_year)\n",
    "\n",
    "df_query_2_2 = df_query_2_2.drop_duplicates(subset=['paper'])\n",
    "result = pd.DataFrame(df_query_2_2.groupby('year')[['descriptive', 'inferential', 'machine_learning', 'method']].count())\n",
    "#display(result)\n",
    "\n",
    "ax = result.plot(kind='bar', rot=0)\n",
    "ax.legend(bbox_to_anchor=(0.85, 0.9), labels=['Descriptive statistic', 'Inferential statistic', 'Machine learning method', 'Other method'])\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Number of empirical methods used for data analysis per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of empirical method used')\n",
    "plt.savefig('Figures/CQ2/da_number_of_empirical_methods_used.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "figure, axis = plt.subplots(2, 2, figsize=(18, 15), sharey=True)\n",
    "# Plot barplots\n",
    "bar1 = sns.barplot(data = result, x = result.index, y='descriptive', ax=axis[0,0], color=sns.color_palette()[0])\n",
    "bar1.bar_label(bar1.containers[0])\n",
    "make_axes_area_auto_adjustable(bar1)\n",
    "bar1.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "bar2 = sns.barplot(data = result, x = result.index, y='inferential', ax=axis[0,1], color=sns.color_palette()[1])\n",
    "bar2.bar_label(bar2.containers[0])\n",
    "make_axes_area_auto_adjustable(bar2)\n",
    "bar2.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "bar3 = sns.barplot(data = result, x = result.index, y='machine_learning', ax=axis[1,0], color=sns.color_palette()[2])\n",
    "bar3.bar_label(bar3.containers[0])\n",
    "make_axes_area_auto_adjustable(bar3)\n",
    "bar3.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "bar4 = sns.barplot(data = result, x = result.index, y='method', ax=axis[1,1], color=sns.color_palette()[3])\n",
    "bar4.bar_label(bar4.containers[0])\n",
    "make_axes_area_auto_adjustable(bar4)\n",
    "bar4.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "# Set titles\n",
    "plt.suptitle(\"Number of empirical method used for data analysis per year grouped by empirical method\")\n",
    "bar1.set(title = 'Number of decriptive statistics used for data analysis per year', ylabel='Number of decriptive statistics', xlabel='Year')\n",
    "bar2.set(title = 'Number of inferential statistics used for data analysis per year', ylabel='Number of inferential statistics', xlabel='Year')\n",
    "bar3.set(title = 'Number of machine learning methods used for data analysis per year', ylabel='Number of machine learning methods', xlabel='Year')\n",
    "bar4.set(title = 'Number of other methods used for data analysis per year', ylabel='Number of other methods', xlabel='Year')\n",
    "\n",
    "# Set spacing between subplots\n",
    "figure.tight_layout()\n",
    "plt.savefig('Figures/CQ2/da_number_of_empirical_methods_used_grouped_by_method.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "result = pd.concat([result, all_papers_per_year], axis=1)\n",
    "result['normalized descriptive'] = (result['descriptive'] / result['number_of_papers_with_da']).round(2)\n",
    "result['normalized inferential'] = (result['inferential'] / result['number_of_papers_with_da']).round(2)\n",
    "result['normalized machine_learning'] = (result['machine_learning'] / result['number_of_papers_with_da']).round(2)\n",
    "result['normalized method'] = (result['method'] / result['number_of_papers_with_da']).round(2)\n",
    "#display(result)\n",
    "\n",
    "ax = result.loc[:, 'normalized descriptive':'normalized method'].plot(kind='bar', rot=0)\n",
    "ax.legend(bbox_to_anchor=(0.6, 0.9), labels=['Descriptive statistic', 'Inferential statistic', 'Machine learning method', 'Other method'])\n",
    "make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Normalized number of empirical methods used for data analysis per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Proportion of empirical method used')\n",
    "plt.savefig('Figures/CQ2/da_proportion_of_empirical_methods_used.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "figure, axis = plt.subplots(2, 2, figsize=(18, 15), sharey=True)\n",
    "# Plot barplots\n",
    "bar1 = sns.barplot(data = result, x = result.index, y='normalized descriptive', ax=axis[0,0], color=sns.color_palette()[0])\n",
    "bar1.bar_label(bar1.containers[0])\n",
    "make_axes_area_auto_adjustable(bar1)\n",
    "bar2 = sns.barplot(data = result, x = result.index, y='normalized inferential', ax=axis[0,1], color=sns.color_palette()[1])\n",
    "bar2.bar_label(bar2.containers[0])\n",
    "make_axes_area_auto_adjustable(bar2)\n",
    "bar3 = sns.barplot(data = result, x = result.index, y='normalized machine_learning', ax=axis[1,0], color=sns.color_palette()[2])\n",
    "bar3.bar_label(bar3.containers[0])\n",
    "make_axes_area_auto_adjustable(bar3)\n",
    "bar4 = sns.barplot(data = result, x = result.index, y='normalized method', ax=axis[1,1], color=sns.color_palette()[3])\n",
    "bar4.bar_label(bar4.containers[0])\n",
    "make_axes_area_auto_adjustable(bar4)\n",
    "\n",
    "# Set titles\n",
    "plt.suptitle(\"Normalized number of empirical methods used for data analysis per year grouped by empirical method\")\n",
    "bar1.set(title = 'Normalized number of decriptive statistics used for data analysis per year', ylabel='Proportion of decriptive statistics', xlabel='Year')\n",
    "bar2.set(title = 'Normalized number of inferential statistics used for data analysis per year', ylabel='Proportion of inferential statistics', xlabel='Year')\n",
    "bar3.set(title = 'Normalized number of machine learning methods used for data analysis per year', ylabel='Proportion of machine learning methods', xlabel='Year')\n",
    "bar4.set(title = 'Normalized number of other methods used for data analysis per year', ylabel='Proportion of other methods', xlabel='Year')\n",
    "\n",
    "# Set spacing between subplots\n",
    "figure.tight_layout()\n",
    "plt.savefig('Figures/CQ2/da_proportion_of_empirical_methods_used_grouped_by_method.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Interpretation*</ins>\n",
    "Muss noch geschrieben werden, wenn die Daten alle geprüft sind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q3'></a>\n",
    "### 3.3 How has the number of papers that do not have an empirical study evolved over time?\n",
    "\n",
    "#### <ins>*Data Selection*</ins> \n",
    "\n",
    "*Explanation of the Competency Question*:\n",
    "\n",
    "For the **target state (2020 - 2025)**, [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30) envision that there should be good reason for **not including** a proper evaluation. This predicted state leads to the corresponding competency question.\n",
    "\n",
    "*Required Data for the Analysis*:\n",
    "\n",
    "We must retrieve all papers with their publication year that use our ORKG template and do not report an empirical study. However, we need to define what we mean by an empirical study. According to [Empirical Software Engineering Journal](https://www.springer.com/journal/10664), \"*Empirical studies presented here usually involve the collection and analysis of data and experience...*\". For this reason, we define that an empirical study is a study that includes data analysis as a necessary condition to be a study (*Necessity*) and data collection as a sufficient condition to be an empirical study (*Sufficiency*). Thus, a study must always include data analysis and an empirical study must include data collection and data analysis. We do not consider the mere reporting of a data collection as a study or even an empirical study. Therefore, we must retrieve all papers that do not have data collection, data analysis, or both.\n",
    "\n",
    "[Back to top](#step3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Collection*</ins> \n",
    "Below, we retrieve the required data from the developed body of knowledge of empirical research in RE from the [ORKG](https://www.orkg.org/orkg/) using its [SPARQL endpoint](https://orkg.org/sparql/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = '3'\n",
    "\n",
    "query_3 = \"\"\"\n",
    "        SELECT ?paper, ?year, ?dc_label, ?da_label\n",
    "        WHERE {\n",
    "                ?paper orkgp:P31 ?contribution;\n",
    "                        orkgp:P29 ?year.\n",
    "                ?contribution a orkgc:C27001.\n",
    "                \n",
    "                OPTIONAL{?contribution orkgp:P56008 ?data_collection.\n",
    "                        ?data_collection rdfs:label ?dc_label.\n",
    "                }\n",
    "                OPTIONAL{?contribution orkgp:P15124 ?data_analysis.\n",
    "                        ?data_analysis rdfs:label ?da_label.\n",
    "                }\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_3)\n",
    "\n",
    "df_query_3 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Exploration*</ins>\n",
    "Below, we explore the retrieved data, including its cleaning and validation, to prepare the data for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_3['dc_label'] = df_query_3['dc_label'].astype('category')\n",
    "df_query_3['da_label'] = df_query_3['da_label'].astype('category')\n",
    "display(df_query_3['dc_label'].value_counts())\n",
    "display(df_query_3['da_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Analysis*</ins>\n",
    "For this data analysis, we select all papers that do not have an empirical study according to our definition (data collection and data analysis). For this reason, we only keep all papers that have \"no collection\" and/or \"no analysis\". For more detailed insights, we normalize the number of all papers without an empirical study based on the number of all unique papers per year, as the total number of papers per year varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(df_query_3.drop_duplicates(subset=['paper']).reset_index(drop=True)['year'].value_counts())\n",
    "result.columns = ['number_of_all_papers']\n",
    "\n",
    "papers_without_emp_study = df_query_3[(df_query_3['dc_label'] == 'no collection') | (df_query_3['da_label'] == 'no analysis')]\n",
    "papers_without_emp_study = papers_without_emp_study.drop_duplicates(subset=['paper'])\n",
    "result['number_of_papers_without_emp_studies'] = papers_without_emp_study.reset_index(drop=True)['year'].value_counts()\n",
    "result['number_of_papers_without_emp_studies'].fillna(0,inplace=True)\n",
    "result['number_of_papers_without_emp_studies'] = result['number_of_papers_without_emp_studies'].astype('int64')\n",
    "\n",
    "result['normalized_papers_without_emp_studies'] = (result['number_of_papers_without_emp_studies'] / result['number_of_all_papers']).round(2)\n",
    "#display(result.sort_index())\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.barplot(data=result, x=result.index, y='number_of_papers_without_emp_studies', color='b')\n",
    "ax.bar_label(ax.containers[0])\n",
    "make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Number of papers without an empirical study per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of papers without an empirical study')\n",
    "plt.savefig('Figures/CQ3/number_of_papers_without_empirical_study.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.barplot(data=result, x=result.index, y='normalized_papers_without_emp_studies', color='b')\n",
    "ax.bar_label(ax.containers[0], fmt='%.2f')\n",
    "make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Normalized number of papers without an empirical study per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Proportion of papers without an empirical study')\n",
    "plt.savefig('Figures/CQ3/proportion_of_papers_without_empirical_study.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Interpretation*</ins>\n",
    "Muss noch geschrieben werden, wenn die Daten alle geprüft sind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q4'></a>\n",
    "### 3.4 How often are which empirical methods used?\n",
    "\n",
    "#### <ins>*Data Selection*</ins> \n",
    "\n",
    "*Explanation of the Competency Question*:\n",
    "\n",
    "For the **target state (2020 - 2025)**, [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30) envision that researchers are trained in using a **large set of research methods and technqiues**. This predicted state leads to the corresponding competency question.\n",
    "\n",
    "*Required Data for the Analysis*:\n",
    "\n",
    "We must retrieve all papers that use our ORKG template and report on the use of empirical methods. According to [Dan (2017)](https://doi.org/10.1002/9781118901731.iecrm0083), empirical methods include data collection and data analysis. An empirical method can therefore be a method for data collection and data analysis. We consider the empirical method used for data collection or data analysis respectively, as our template allows for a correspondingly more fine-grained analysis.\n",
    "\n",
    "[Back to top](#step3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Collection*</ins> \n",
    "Below, we retrieve the required data from the developed body of knowledge of empirical research in RE from the [ORKG](https://www.orkg.org/orkg/) using its [SPARQL endpoint](https://orkg.org/sparql/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = '4.1'\n",
    "\n",
    "query_4_1_data_collection = \"\"\"\n",
    "        SELECT ?paper, ?dc_label\n",
    "        WHERE {\n",
    "                ?paper orkgp:P31 ?contribution.\n",
    "                ?contribution a orkgc:C27001.\n",
    "\n",
    "                OPTIONAL{?contribution orkgp:P56008 ?data_collection.\n",
    "                        ?data_collection rdfs:label ?dc_label.\n",
    "                }\n",
    "                FILTER(?dc_label != \"no collection\"^^xsd:string)\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_4_1_data_collection)\n",
    "df_query_4_1 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')\n",
    "\n",
    "ID = '4.2'\n",
    "query_4_2_data_analysis = \"\"\"\n",
    "        SELECT ?paper, ?da_label, ?descriptive, ?inferential, ?machine_learning, ?method\n",
    "        WHERE {\n",
    "                ?paper orkgp:P31 ?contribution.\n",
    "                ?contribution a orkgc:C27001.\n",
    "                                \n",
    "                OPTIONAL{?contribution orkgp:P15124 ?data_analysis.\n",
    "                        ?data_analysis rdfs:label ?da_label.\n",
    "                        \n",
    "                        OPTIONAL{?data_analysis orkgp:P56048/rdfs:label ?descriptive.}\n",
    "                        OPTIONAL{?data_analysis orkgp:P56043/rdfs:label ?inferential.}\n",
    "                        OPTIONAL{?data_analysis orkgp:P57016/rdfs:label ?machine_learning.}\n",
    "                        OPTIONAL{?data_analysis orkgp:P76003/rdfs:label ?method}\n",
    "                }\n",
    "                FILTER(?da_label != \"no analysis\"^^xsd:string)\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_4_2_data_analysis)\n",
    "df_query_4_2 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Exploration*</ins>\n",
    "Below, we explore the retrieved data, including its cleaning and validation, to prepare the data for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_4_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_4_1['dc_label'] = df_query_4_1['dc_label'].astype('category')\n",
    "display(df_query_4_1['dc_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_4_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_4_2['da_label'] = df_query_4_2['da_label'].astype('category')\n",
    "df_query_4_2['descriptive'] = df_query_4_2['descriptive'].astype('category')\n",
    "df_query_4_2['inferential'] = df_query_4_2['inferential'].astype('category')\n",
    "df_query_4_2['machine_learning'] = df_query_4_2['machine_learning'].astype('category')\n",
    "df_query_4_2['method'] = df_query_4_2['method'].astype('category')\n",
    "\n",
    "display(df_query_4_2['da_label'].value_counts())\n",
    "display(df_query_4_2['descriptive'].value_counts())\n",
    "display(df_query_4_2['inferential'].value_counts())\n",
    "display(df_query_4_2['machine_learning'].value_counts())\n",
    "display(df_query_4_2['method'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Analysis*</ins>\n",
    "For this data analysis, we consider the empirical methods used for data collection. We identify the empirical methods used for data collection. A paper can involve more than one empirical method for data collection so that the number of empirical methods can be larger than the number of papers using empirical methods for data collection. We normalize the number of empirical methods used for data collection based on the number of all papers using at least one empirical methods for data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(df_query_4_1['dc_label'].value_counts())\n",
    "result.index = result.index.map(str.capitalize)\n",
    "result = result.rename(index={'Study': 'Other method'})\n",
    "other_methods = result.loc['Other method']\n",
    "result = result.drop(index='Other method')\n",
    "result = result.append(other_methods)\n",
    "#display(result)\n",
    "\n",
    "ax = result.plot(kind='barh', rot=0)\n",
    "ax.invert_yaxis()\n",
    "ax.bar_label(ax.containers[0])\n",
    "ax.get_legend().remove()\n",
    "make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Number of empirical methods used for data collection')\n",
    "plt.xlabel('Number of empirical method used')\n",
    "plt.ylabel('Empirical method used')\n",
    "plt.savefig('Figures/CQ4/dc_number_of_empirical_methods_used.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "number_of_all_papers_with_dc = result['dc_label'].sum()\n",
    "result['normalized'] = (result['dc_label'] / number_of_all_papers_with_dc).round(3)\n",
    "#display(result)\n",
    "\n",
    "plt.figure()\n",
    "ax = result.loc[:, 'normalized'].plot(kind='barh', rot=0)\n",
    "ax.invert_yaxis()\n",
    "ax.bar_label(ax.containers[0])\n",
    "make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Normalized number of empirical methods used for data collection')\n",
    "plt.xlabel('Proportion of empirical method used')\n",
    "plt.ylabel('Empirical method used')\n",
    "plt.savefig('Figures/CQ4/dc_proportion_of_empirical_methods_used.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data analysis, we consider the empirical methods used for data analysis. We identify the empirical methods used for data analysis. A paper can involve more than one empirical method for data analysis so that the number of empirical methods can be larger than the number of papers using empirical methods for data analysis. We normalize the number of empirical methods used for data analysis based on the number of all papers using at least one empirical methods for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_4_2 = df_query_4_2.drop_duplicates(subset=['paper'])\n",
    "result = pd.DataFrame(df_query_4_2[['descriptive', 'inferential', 'machine_learning', 'method']].count(), columns=['number of method'])\n",
    "result = result.rename(index={'descriptive': 'Descriptive statistics', 'inferential': 'Inferential statistics', 'machine_learning': 'Machine learning method','method': 'Other method'})\n",
    "result.index = result.index.map(str.capitalize)\n",
    "#display(result)\n",
    "\n",
    "ax = result.plot(kind='barh', rot=0)\n",
    "ax.invert_yaxis()\n",
    "ax.bar_label(ax.containers[0])\n",
    "ax.get_legend().remove()\n",
    "make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Number of empirical methods used for data analysis')\n",
    "plt.xlabel('Number of empirical method used')\n",
    "plt.ylabel('Empirical method used')\n",
    "plt.savefig('Figures/CQ4/da_number_of_empirical_methods_used.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "number_of_all_papers_with_da = df_query_4_2.count()['da_label']\n",
    "result['normalized'] = (result['number of method'] / number_of_all_papers_with_da).round(2)\n",
    "#display(result)\n",
    "\n",
    "plt.figure()\n",
    "ax = result.loc[:, 'normalized'].plot(kind='barh', rot=0)\n",
    "ax.invert_yaxis()\n",
    "xmin, xmax = plt.xlim()\n",
    "plt.xlim(xmin, xmax+0.01)\n",
    "ax.bar_label(ax.containers[0])\n",
    "make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Normalized number of empirical methods used for data analysis')\n",
    "plt.xlabel('Proportion of empirical method used')\n",
    "plt.ylabel('Empirical method used')\n",
    "plt.savefig('Figures/CQ4/da_proportion_of_empirical_methods_used.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Interpretation*</ins>\n",
    "Muss noch geschrieben werden, wenn die Daten alle geprüft sind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q5'></a>\n",
    "### 3.5 How have the proportions of experiments, secondary research (reviews), surveys, case studies, and action research in the empirical methods used evolved over time?\n",
    "\n",
    "#### <ins>*Data Selection*</ins> \n",
    "\n",
    "*Explanation of the Competency Question*:\n",
    "\n",
    "According to [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30), the **\"current\" state of practice (2007)** shows that **researchers are skilled** (frequently use) **experiments and reviews** (secondary research), but **researchers are not skilled** (do not frequently use) **surveys, case studies, and action research**. For the **target state (2020 - 2025)**, [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30) envision that researchers are trained in using **large set of research methods and technqiues** and that their **skills** (frequent use) **of conducting case studies and actions research are stimulated**. This predicted change leads to the corresponding competency question.\n",
    "\n",
    "*Required Data for the Analysis*:\n",
    "\n",
    "We must retrieve all papers with their publication year that use our ORKG template and report on the use of the empirical methods: experiments, secondary reserach (reviews), surveys, case studies, and action research.\n",
    "\n",
    "[Back to top](#step3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Collection*</ins> \n",
    "Below, we retrieve the required data from the developed body of knowledge of empirical research in RE from the [ORKG](https://www.orkg.org/orkg/) using its [SPARQL endpoint](https://orkg.org/sparql/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = '5'\n",
    "\n",
    "query_5_data_collection = \"\"\"\n",
    "        SELECT ?paper, ?year, ?dc_label\n",
    "        WHERE {\n",
    "                ?paper orkgp:P31 ?contribution;\n",
    "                        orkgp:P29 ?year.\n",
    "                ?contribution a orkgc:C27001.\n",
    "\n",
    "                OPTIONAL{?contribution orkgp:P56008 ?data_collection.\n",
    "                        ?data_collection rdfs:label ?dc_label.\n",
    "                }\n",
    "                FILTER(?dc_label != \"no collection\"^^xsd:string)\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_5_data_collection)\n",
    "df_query_5 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Exploration*</ins>\n",
    "Below, we explore the retrieved data, including its cleaning and validation, to prepare the data for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_5['dc_label'] = df_query_5['dc_label'].astype('category')\n",
    "display(df_query_5['dc_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Analysis*</ins>\n",
    "For this data analysis, we consider all papers that report on the use of the empirical methods: experiment, secondary research, survey, case study, action research. A paper can involve more than one empirical method for data collection so that the number of empirical methods can be larger than the number of papers. We normalize the number of empirical methods (experiment, secondary research, survey, case study, action research) used based on the number of all unique papers per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers_per_year = pd.DataFrame(df_query_5.drop_duplicates(subset=['paper']).reset_index(drop=True)['year'].value_counts())\n",
    "all_papers_per_year.columns = ['number_of_papers_with_dc']\n",
    "all_papers_per_year = all_papers_per_year.sort_index()\n",
    "\n",
    "result = pd.DataFrame(df_query_5.groupby('year')['dc_label'].value_counts().unstack())\n",
    "result = result[result.sum().sort_values(ascending=False).index]\n",
    "#display(result)\n",
    "\n",
    "result = pd.concat([result, all_papers_per_year], axis=1)\n",
    "result['normalized experiment'] = (result['experiment'] / result['number_of_papers_with_dc']).round(2)\n",
    "result['normalized case study'] = (result['case study'] / result['number_of_papers_with_dc']).round(2)\n",
    "result['normalized secondary research'] = (result['secondary research'] / result['number_of_papers_with_dc']).round(2)\n",
    "result['normalized survey'] = (result['survey'] / result['number_of_papers_with_dc']).round(2)\n",
    "result['normalized action research'] = (result['action research'] / result['number_of_papers_with_dc']).round(2)\n",
    "#display(result)\n",
    "\n",
    "ax = result.loc[:, 'normalized experiment':'normalized action research'].plot(kind='bar', rot=0)\n",
    "ax.legend(bbox_to_anchor=(0.4, 0.65), labels=['Experiment', 'Case study', 'Secondary research', 'Survey', 'Action research',])\n",
    "make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Normalized number of empirical methods used for data collection per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Proportion of empirical method used')\n",
    "plt.savefig('Figures/CQ5/dc_proportion_of_empirical_methods_used.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "figure, axis = plt.subplots(3, 2, figsize=(18, 15), sharey=True)\n",
    "# Plot barplots\n",
    "bar1 = sns.barplot(data = result, x = result.index, y='normalized experiment', ax=axis[0,0], color=sns.color_palette()[0])\n",
    "bar1.bar_label(bar1.containers[0])\n",
    "make_axes_area_auto_adjustable(bar1)\n",
    "bar2 = sns.barplot(data = result, x = result.index, y='normalized case study', ax=axis[0,1], color=sns.color_palette()[1])\n",
    "bar2.bar_label(bar2.containers[0])\n",
    "make_axes_area_auto_adjustable(bar2)\n",
    "bar3 = sns.barplot(data = result, x = result.index, y='normalized secondary research', ax=axis[1,0], color=sns.color_palette()[2])\n",
    "bar3.bar_label(bar3.containers[0])\n",
    "make_axes_area_auto_adjustable(bar3)\n",
    "bar4 = sns.barplot(data = result, x = result.index, y='normalized survey', ax=axis[1,1], color=sns.color_palette()[4])\n",
    "bar4.bar_label(bar4.containers[0])\n",
    "make_axes_area_auto_adjustable(bar4)\n",
    "bar5 = sns.barplot(data = result, x = result.index, y='normalized action research', ax=axis[2,0], color=sns.color_palette()[5])\n",
    "bar5.bar_label(bar5.containers[0])\n",
    "make_axes_area_auto_adjustable(bar5)\n",
    "\n",
    "# Remove unnecessary subplot\n",
    "figure.delaxes(axis[2,1])\n",
    "\n",
    "# Set titles\n",
    "plt.suptitle(\"Normalized number of empirical methods used for data collection per year grouped by empirical method\")\n",
    "bar1.set(title = 'Normalized number of experiments used for data collection per year', ylabel='Proportion of experiments', xlabel='Year')\n",
    "bar2.set(title = 'Normalized number of case studies used for data collection per year', ylabel='Proportion of case studies', xlabel='Year')\n",
    "bar3.set(title = 'Normalized number of secondary research used for data collection per year', ylabel='Proportion of secondary research', xlabel='Year')\n",
    "bar4.set(title = 'Normalized number of interviews used for data collection per year', ylabel='Proportion of interviews', xlabel='Year')\n",
    "bar5.set(title = 'Normalized number of surveys used for data collection per year', ylabel='Proportion of surveys', xlabel='Year')\n",
    "bar6.set(title = 'Normalized number of actions research used for data collection per year', ylabel='Proportion of action research', xlabel='Year')\n",
    "bar7.set(title = 'Normalized number of other methods used for data collection per year', ylabel='Proportion of other methods', xlabel='Year')\n",
    "\n",
    "# Set spacing between subplots\n",
    "figure.tight_layout()\n",
    "plt.savefig('Figures/CQ5/dc_proportion_of_empirical_methods_used_grouped_by_method.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Interpretation*</ins>\n",
    "Muss noch geschrieben werden, wenn die Daten alle geprüft sind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q6'></a>\n",
    "### 3.6 How often are which statistical methods used?\n",
    "\n",
    "#### <ins>*Data Selection*</ins> \n",
    "\n",
    "*Explanation of the Competency Question*:\n",
    "\n",
    "According to [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30), the **\"current\" state of practice (2007)** shows that **statistical methods are used mechanically, and with little knowledge about limitations and assumptions**. For the **target state (2020 - 2025)**, [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30) envision that **the use of statistical methods is mature**. This predicted change leads to the corresponding competency question.\n",
    "\n",
    "*Required Data for the Analysis*:\n",
    "\n",
    "We must retrieve all papers that use our ORKG template and report on the use of the statistical methods: descriptive statistics and inferential statistics. We consider each of these two types of statistics separately, as our template allows for a correspondingly more fine-grained analysis.\n",
    "\n",
    "[Back to top](#step3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Collection*</ins> \n",
    "Below, we retrieve the required data from the developed body of knowledge of empirical research in RE from the [ORKG](https://www.orkg.org/orkg/) using its [SPARQL endpoint](https://orkg.org/sparql/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = '6.1'\n",
    "\n",
    "query_6_1_des_stats = \"\"\"\n",
    "        SELECT ?paper, ?da_label, ?count, ?percent, ?mean, ?median, ?mode, ?minimum, ?maximum, \n",
    "               ?range, ?variance, ?standard_deviation, ?percentile_rank, ?quartile_rank, ?boxplot\n",
    "        WHERE {\n",
    "        ?paper orkgp:P31 ?contribution.\n",
    "        ?contribution a orkgc:C27001.\n",
    "\n",
    "        OPTIONAL{?contribution orkgp:P15124 ?data_analysis.\n",
    "                ?data_analysis rdfs:label ?da_label.\n",
    "                \n",
    "                OPTIONAl{?data_analysis orkgp:P56048 ?descriptive_stats.\n",
    "                        OPTIONAL{?descriptive_stats orkgp:P56049 ?frequency.\n",
    "                                OPTIONAL{?frequency orkgp:P55023 ?count.}\n",
    "                                OPTIONAL{?frequency orkgp:P56050 ?percent.}}\n",
    "                        OPTIONAL{?descriptive_stats orkgp:P57005 ?central_tendency.\n",
    "                                OPTIONAL{?central_tendency orkgp:P47000 ?mean.}\n",
    "                                OPTIONAL{?central_tendency orkgp:P57006 ?median.}\n",
    "                                OPTIONAL{?central_tendency orkgp:P57007 ?mode.}\n",
    "                                OPTIONAL{?central_tendency orkgp:P44107 ?minimum.}\n",
    "                                OPTIONAL{?central_tendency orkgp:P44108 ?maximum.}}\n",
    "                        OPTIONAL{?descriptive_stats orkgp:P57008 ?variation.\n",
    "                                OPTIONAL{?variation orkgp:P4013 ?range.}\n",
    "                                OPTIONAL{?variation orkgp:P57009 ?variance.}\n",
    "                                OPTIONAL{?variation orkgp:P44087 ?standard_deviation.}}\n",
    "                        OPTIONAL{?descriptive_stats orkgp:P57010 ?position.\n",
    "                                OPTIONAL{?position orkgp:P57011 ?percentile_rank.}\n",
    "                                OPTIONAL{?position orkgp:P57012 ?quartile_rank.}\n",
    "                                OPTIONAL{?position orkgp:P59065 ?boxplot.}}\n",
    "                }\n",
    "        }\n",
    "        FILTER(?da_label = 'analysis'^^xsd:string)\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_6_1_des_stats)\n",
    "df_query_6_1 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')\n",
    "\n",
    "ID = '6.2'\n",
    "query_6_2_inf_stats = \"\"\"\n",
    "        SELECT ?paper, ?da_label, ?test\n",
    "        WHERE {\n",
    "                ?paper orkgp:P31 ?contribution.\n",
    "                ?contribution a orkgc:C27001.\n",
    "\n",
    "                OPTIONAL{?contribution orkgp:P15124 ?data_analysis.\n",
    "                        ?data_analysis rdfs:label ?da_label.\n",
    "                        \n",
    "                        OPTIONAl{?data_analysis orkgp:P56043 ?inferential_stats.\n",
    "                                OPTIONAL{?inferential_stats orkgp:P35133 ?stats_test.\n",
    "                                        ?stats_test rdfs:label ?test}\n",
    "                        }\n",
    "                }\n",
    "                FILTER(?da_label = 'analysis'^^xsd:string)\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_6_2_inf_stats)\n",
    "df_query_6_2 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Exploration*</ins>\n",
    "Below, we explore the retrieved data, including its cleaning and validation, to prepare the data for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_6_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_6_1['da_label'] = df_query_6_1['da_label'].astype('category')\n",
    "df_query_6_1['count'] = df_query_6_1['count'].fillna(False).astype(bool)\n",
    "df_query_6_1['percent'] = df_query_6_1['percent'].fillna(False).astype(bool)\n",
    "df_query_6_1['mean'] = df_query_6_1['mean'].fillna(False).astype(bool)\n",
    "df_query_6_1['median'] = df_query_6_1['median'].fillna(False).astype(bool)\n",
    "df_query_6_1['mode'] = df_query_6_1['mode'].fillna(False).astype(bool)\n",
    "df_query_6_1['minimum'] = df_query_6_1['minimum'].fillna(False).astype(bool)\n",
    "df_query_6_1['maximum'] = df_query_6_1['maximum'].fillna(False).astype(bool)\n",
    "df_query_6_1['range'] = df_query_6_1['range'].fillna(False).astype(bool)\n",
    "df_query_6_1['variance'] = df_query_6_1['variance'].fillna(False).astype(bool)\n",
    "df_query_6_1['standard_deviation'] = df_query_6_1['standard_deviation'].fillna(False).astype(bool)\n",
    "df_query_6_1['percentile_rank'] = df_query_6_1['percentile_rank'].fillna(False).astype(bool)\n",
    "df_query_6_1['quartile_rank'] = df_query_6_1['quartile_rank'].fillna(False).astype(bool)\n",
    "df_query_6_1['boxplot'] = df_query_6_1['boxplot'].fillna(False).astype(bool)\n",
    "#display(df_query_6_1.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_6_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_6_2['da_label'] = df_query_6_2['da_label'].astype('category')\n",
    "df_query_6_2['test'] = df_query_6_2['test'].astype('category')\n",
    "\n",
    "#display(df_query_6_2['da_label'].value_counts())\n",
    "#display(df_query_6_2['test'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Analysis*</ins>\n",
    "For this data analysis, we consider the statistical methods of descriptive statistics used for data analysis. We identify the statistical methods of descriptive statistics used for data analysis. A paper can involve more than one statistical methods of descriptive statistics used for data analysis so that the number of statistical methodsof descriptive statistics used for data analysis can be larger than the number of papers using empricial methods for data analysis. We normalize the number of statistical methods of descriptive statistics used for data analysis based on the number of all papers using at least one empirical method for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(df_query_6_1[['count', 'percent','mean', 'median', 'mode', 'minimum', 'maximum', 'range', 'variance', 'standard_deviation', 'percentile_rank', 'quartile_rank', 'boxplot']].sum().sort_values())\n",
    "result.columns = ['number_of_papers_with_des_stats']\n",
    "result.index = result.index.map(str.capitalize)\n",
    "result = result.rename(index={'Standard_deviation': 'Standard deviation', 'Percentile_rank': 'Percentile rank', 'Quartile_rank': 'Quartile rank'})\n",
    "#display(result)\n",
    "\n",
    "ax = result.plot(kind='barh', rot=0)\n",
    "#ax.invert_yaxis()\n",
    "ax.bar_label(ax.containers[0])\n",
    "ax.get_legend().remove()\n",
    "make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Number of statistical methods of descriptive statistics used for data analysis')\n",
    "plt.xlabel('Number of statistical method used')\n",
    "plt.ylabel('Statistical method used')\n",
    "plt.savefig('Figures/CQ6/des_stats_number_of_statistical_methods_used.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "number_of_all_papers_with_da = df_query_6_1['da_label'].count()\n",
    "result['normalized'] = (result['number_of_papers_with_des_stats'] / number_of_all_papers_with_da).round(2)\n",
    "#display(result)\n",
    "\n",
    "plt.figure()\n",
    "ax = result.loc[:, 'normalized'].plot(kind='barh', rot=0)\n",
    "ax.bar_label(ax.containers[0])\n",
    "make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Normalized number of statistical methods of descriptive statistics used for data analysis')\n",
    "plt.xlabel('Proportion of statistical method used')\n",
    "plt.ylabel('Statistical method used')\n",
    "plt.savefig('Figures/CQ6/des_stats_proportion_of_statistical_methods_used.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data analysis, we consider the statistical methods of inferential statistics used for data analysis. We identify the statistical methods of inferenttial statistics used for data analysis. A paper can involve more than one statistical methods of inferential statistics used for data analysis so that the number of statistical methodsof inferential statistics used for data analysis can be larger than the number of papers using empricial methods for data analysis. We normalize the number of statistical methods of inferential statistics used for data analysis based on the number of all papers using at least one empirical method for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(df_query_6_2['test'].value_counts())\n",
    "result.index = result.index.map(str.capitalize)\n",
    "#display(result)\n",
    "\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_area_auto_adjustable\n",
    "\n",
    "ax = result.plot(kind='barh', rot=0)\n",
    "ax.invert_yaxis()\n",
    "ax.bar_label(ax.containers[0])\n",
    "ax.get_legend().remove()\n",
    "make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Number of statistical methods of inferential statistics used for data analysis')\n",
    "plt.xlabel('Number of statistical method used')\n",
    "plt.ylabel('Statistical method used')\n",
    "plt.savefig('Figures/CQ6/inf_stats_number_of_statistical_methods_used.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "number_of_all_papers_with_da = df_query_6_2.drop_duplicates(subset=['paper'])['da_label'].count()\n",
    "result['normalized'] = (result['test'] / number_of_all_papers_with_da).round(3)\n",
    "#display(result)\n",
    "\n",
    "plt.figure()\n",
    "ax = result.loc[:, 'normalized'].plot(kind='barh', rot=0)\n",
    "ax.invert_yaxis()\n",
    "ax.bar_label(ax.containers[0])\n",
    "make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Normalized of statistical methods of inferential statistics used for data analysis')\n",
    "plt.xlabel('Proportion of statistical method used')\n",
    "plt.ylabel('Statistical method used')\n",
    "plt.savefig('Figures/CQ6/inf_stats_proportion_of_statistical_methods_used.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Interpretation*</ins>\n",
    "Muss noch geschrieben werden, wenn die Daten alle geprüft sind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q7'></a>\n",
    "### 3.7 How has the use of statistical methods evolved over time?\n",
    "\n",
    "#### <ins>*Data Selection*</ins> \n",
    "\n",
    "*Explanation of the Competency Question*:\n",
    "\n",
    "According to [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30), the **\"current\" state of practice (2007)** shows that **statistical methods are used mechanically, and with little knowledge about limitations and assumptions**. For the **target state (2020 - 2025)**, [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30) envision that **the use of statistical methods is mature**. This predicted change leads to the corresponding competency question.\n",
    "\n",
    "*Required Data for the Analysis*:\n",
    "\n",
    "We must retrieve all papers with their publication year that use our ORKG template and report on the use of the statistical methods: descriptive statistics and inferential statistics. We consider each of these two types of statistics separately, as our template allows for a correspondingly more fine-grained analysis.\n",
    "\n",
    "[Back to top](#step3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Collection*</ins> \n",
    "Below, we retrieve the required data from the developed body of knowledge of empirical research in RE from the [ORKG](https://www.orkg.org/orkg/) using its [SPARQL endpoint](https://orkg.org/sparql/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = '7.1'\n",
    "\n",
    "query_7_1_des_stats = \"\"\"\n",
    "        SELECT ?paper, ?year, ?da_label, ?count, ?percent, ?mean, ?median, ?mode, ?minimum, ?maximum, \n",
    "               ?range, ?variance, ?standard_deviation, ?percentile_rank, ?quartile_rank, ?boxplot\n",
    "        WHERE {\n",
    "        ?paper orkgp:P31 ?contribution;\n",
    "               orkgp:P29 ?year.\n",
    "        ?contribution a orkgc:C27001.\n",
    "\n",
    "        OPTIONAL{?contribution orkgp:P15124 ?data_analysis.\n",
    "                ?data_analysis rdfs:label ?da_label.\n",
    "                \n",
    "                OPTIONAl{?data_analysis orkgp:P56048 ?descriptive_stats.\n",
    "                        OPTIONAL{?descriptive_stats orkgp:P56049 ?frequency.\n",
    "                                OPTIONAL{?frequency orkgp:P55023 ?count.}\n",
    "                                OPTIONAL{?frequency orkgp:P56050 ?percent.}}\n",
    "                        OPTIONAL{?descriptive_stats orkgp:P57005 ?central_tendency.\n",
    "                                OPTIONAL{?central_tendency orkgp:P47000 ?mean.}\n",
    "                                OPTIONAL{?central_tendency orkgp:P57006 ?median.}\n",
    "                                OPTIONAL{?central_tendency orkgp:P57007 ?mode.}\n",
    "                                OPTIONAL{?central_tendency orkgp:P44107 ?minimum.}\n",
    "                                OPTIONAL{?central_tendency orkgp:P44108 ?maximum.}}\n",
    "                        OPTIONAL{?descriptive_stats orkgp:P57008 ?variation.\n",
    "                                OPTIONAL{?variation orkgp:P4013 ?range.}\n",
    "                                OPTIONAL{?variation orkgp:P57009 ?variance.}\n",
    "                                OPTIONAL{?variation orkgp:P44087 ?standard_deviation.}}\n",
    "                        OPTIONAL{?descriptive_stats orkgp:P57010 ?position.\n",
    "                                OPTIONAL{?position orkgp:P57011 ?percentile_rank.}\n",
    "                                OPTIONAL{?position orkgp:P57012 ?quartile_rank.}\n",
    "                                OPTIONAL{?position orkgp:P59065 ?boxplot.}}\n",
    "                }\n",
    "        }\n",
    "        FILTER(?da_label = 'analysis'^^xsd:string)\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_7_1_des_stats)\n",
    "df_query_7_1 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')\n",
    "\n",
    "ID = '7.2'\n",
    "query_7_2_inf_stats = \"\"\"\n",
    "        SELECT ?paper, ?year, ?da_label, ?test\n",
    "        WHERE {\n",
    "                ?paper orkgp:P31 ?contribution;\n",
    "                        orkgp:P29 ?year.\n",
    "                ?contribution a orkgc:C27001.\n",
    "\n",
    "                OPTIONAL{?contribution orkgp:P15124 ?data_analysis.\n",
    "                        ?data_analysis rdfs:label ?da_label.\n",
    "                        \n",
    "                        OPTIONAl{?data_analysis orkgp:P56043 ?inferential_stats.\n",
    "                                OPTIONAL{?inferential_stats orkgp:P35133 ?stats_test.\n",
    "                                        ?stats_test rdfs:label ?test}\n",
    "                        }\n",
    "                }\n",
    "                FILTER(?da_label = 'analysis'^^xsd:string)\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_7_2_inf_stats)\n",
    "df_query_7_2 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Exploration*</ins>\n",
    "Below, we explore the retrieved data, including its cleaning and validation, to prepare the data for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_7_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_7_1['da_label'] = df_query_7_1['da_label'].astype('category')\n",
    "df_query_7_1['count'] = df_query_7_1['count'].fillna(False).astype(bool)\n",
    "df_query_7_1['percent'] = df_query_7_1['percent'].fillna(False).astype(bool)\n",
    "df_query_7_1['mean'] = df_query_7_1['mean'].fillna(False).astype(bool)\n",
    "df_query_7_1['median'] = df_query_7_1['median'].fillna(False).astype(bool)\n",
    "df_query_7_1['mode'] = df_query_7_1['mode'].fillna(False).astype(bool)\n",
    "df_query_7_1['minimum'] = df_query_7_1['minimum'].fillna(False).astype(bool)\n",
    "df_query_7_1['maximum'] = df_query_7_1['maximum'].fillna(False).astype(bool)\n",
    "df_query_7_1['range'] = df_query_7_1['range'].fillna(False).astype(bool)\n",
    "df_query_7_1['variance'] = df_query_7_1['variance'].fillna(False).astype(bool)\n",
    "df_query_7_1['standard_deviation'] = df_query_7_1['standard_deviation'].fillna(False).astype(bool)\n",
    "df_query_7_1['percentile_rank'] = df_query_7_1['percentile_rank'].fillna(False).astype(bool)\n",
    "df_query_7_1['quartile_rank'] = df_query_7_1['quartile_rank'].fillna(False).astype(bool)\n",
    "df_query_7_1['boxplot'] = df_query_7_1['boxplot'].fillna(False).astype(bool)\n",
    "#display(df_query_7_1.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_7_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_7_2['da_label'] = df_query_7_2['da_label'].astype('category')\n",
    "df_query_7_2['test'] = df_query_7_2['test'].astype('category')\n",
    "\n",
    "#display(df_query_7_2['da_label'].value_counts())\n",
    "#display(df_query_7_2['test'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Analysis*</ins>\n",
    "For this data analysis, we consider the statistical methods of descriptive statistics used for data analysis. We identify the statistical methods of descriptive statistics used for data analysis. A paper can involve more than one statistical methods of descriptive statistics used for data analysis so that the number of statistical methodsof descriptive statistics used for data analysis can be larger than the number of papers using empricial methods for data analysis. We normalize the number of statistical methods of descriptive statistics used for data analysis based on the number of all papers using at least one empirical method for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(df_query_7_1.groupby('year').sum())\n",
    "#display(result)\n",
    "\n",
    "ax = result.plot(kind='bar', rot=0)\n",
    "ax.legend(bbox_to_anchor=(0.7, 0.3), labels=['Count', 'Percent', 'Mean', 'Median', 'Mode', 'Minimum', 'Maximum', 'Range', 'Variance', 'Standard deviation', 'Percentile rank', 'Quartile rank', 'Boxplot'])\n",
    "#make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Number of statistical methods of descriptive statistics used for data analysis per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of statistical method used')\n",
    "plt.savefig('Figures/CQ7/des_stats_number_of_statistical_methods_used.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "figure, axis = plt.subplots(7, 2, figsize=(18, 22), sharey=True)\n",
    "# Plot barplots\n",
    "bar1 = sns.barplot(data = result, x = result.index, y='count', ax=axis[0,0], color=sns.color_palette()[0])\n",
    "bar1.bar_label(bar1.containers[0])\n",
    "#make_axes_area_auto_adjustable(bar1)\n",
    "bar2 = sns.barplot(data = result, x = result.index, y='percent', ax=axis[0,1], color=sns.color_palette()[1])\n",
    "bar2.bar_label(bar2.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar2)\n",
    "bar3 = sns.barplot(data = result, x = result.index, y='mean', ax=axis[1,0], color=sns.color_palette()[2])\n",
    "bar3.bar_label(bar3.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar3)\n",
    "bar4 = sns.barplot(data = result, x = result.index, y='median', ax=axis[1,1], color=sns.color_palette()[3])\n",
    "bar4.bar_label(bar4.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar4)\n",
    "bar5 = sns.barplot(data = result, x = result.index, y='mode', ax=axis[2,0], color=sns.color_palette()[4])\n",
    "bar5.bar_label(bar5.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar5)\n",
    "bar6 = sns.barplot(data = result, x = result.index, y='minimum', ax=axis[2,1], color=sns.color_palette()[5])\n",
    "bar6.bar_label(bar6.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar6)\n",
    "bar7 = sns.barplot(data = result, x = result.index, y='maximum', ax=axis[3,0], color=sns.color_palette()[6])\n",
    "bar7.bar_label(bar7.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar7)\n",
    "bar8 = sns.barplot(data = result, x = result.index, y='range', ax=axis[3,1], color=sns.color_palette()[7])\n",
    "bar8.bar_label(bar8.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar7)\n",
    "bar9 = sns.barplot(data = result, x = result.index, y='variance', ax=axis[4,0], color=sns.color_palette()[8])\n",
    "bar9.bar_label(bar9.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar7)\n",
    "bar10 = sns.barplot(data = result, x = result.index, y='standard_deviation', ax=axis[4,1], color=sns.color_palette()[9])\n",
    "bar10.bar_label(bar10.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar7)\n",
    "bar11 = sns.barplot(data = result, x = result.index, y='percentile_rank', ax=axis[5,0], color=sns.color_palette()[0])\n",
    "bar11.bar_label(bar11.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar7)\n",
    "bar12 = sns.barplot(data = result, x = result.index, y='quartile_rank', ax=axis[5,1], color=sns.color_palette()[1])\n",
    "bar12.bar_label(bar12.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar7)\n",
    "bar13 = sns.barplot(data = result, x = result.index, y='boxplot', ax=axis[6,0], color=sns.color_palette()[2])\n",
    "bar13.bar_label(bar13.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar7)\n",
    "\n",
    "# Remove unnecessary subplot\n",
    "figure.delaxes(axis[6,1])\n",
    "\n",
    "# Set titles\n",
    "plt.suptitle(\"Number of statistical method used for data analysis per year grouped by statistical method\")\n",
    "bar1.set(title = 'Number of count method used for data analysis per year', ylabel='Number of count method', xlabel='Year')\n",
    "bar2.set(title = 'Number of percent method used for data analysis per year', ylabel='Number of percent method', xlabel='Year')\n",
    "bar3.set(title = 'Number of mean method used for data analysis per year', ylabel='Number of mean method', xlabel='Year')\n",
    "bar4.set(title = 'Number of median method used for data analysis per year', ylabel='Number of median method', xlabel='Year')\n",
    "bar5.set(title = 'Number of mode method used for data analysis per year', ylabel='Number of mode method', xlabel='Year')\n",
    "bar6.set(title = 'Number of minimum method used for data analyis per year', ylabel='Number of minimum method', xlabel='Year')\n",
    "bar7.set(title = 'Number of maximum method used for data analysis per year', ylabel='Number of maximum method', xlabel='Year')\n",
    "bar8.set(title = 'Number of range method used for data analysis per year', ylabel='Number of range method', xlabel='Year')\n",
    "bar9.set(title = 'Number of variance method used for data analysis per year', ylabel='Number of variance method', xlabel='Year')\n",
    "bar10.set(title = 'Number of standard deviation method used for data analysis per year', ylabel='Number of standard deviation method', xlabel='Year')\n",
    "bar11.set(title = 'Number of percentile rank method used for data analysis per year', ylabel='Number of percentile rank method', xlabel='Year')\n",
    "bar12.set(title = 'Number of quartile rank method used for data analysis per year', ylabel='Number of quartile rank method', xlabel='Year')\n",
    "bar13.set(title = 'Number of boxplot method used for data analysis per year', ylabel='Number of boxplot method', xlabel='Year')\n",
    "\n",
    "# Set spacing between subplots\n",
    "figure.tight_layout()\n",
    "plt.savefig('Figures/CQ7/des_stats_number_of_statistical_methods_used_grouped_by_method.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "number_of_all_papers_with_da = df_query_7_1.groupby('year')['da_label'].count()\n",
    "\n",
    "result = pd.concat([result, number_of_all_papers_with_da], axis=1)\n",
    "result['normalized count'] = (result['count'] / result['da_label']).round(2)\n",
    "result['normalized percent'] = (result['percent'] / result['da_label']).round(2)\n",
    "result['normalized mean'] = (result['mean'] / result['da_label']).round(2)\n",
    "result['normalized median'] = (result['median'] / result['da_label']).round(2)\n",
    "result['normalized mode'] = (result['mode'] / result['da_label']).round(2)\n",
    "result['normalized minimum'] = (result['minimum'] / result['da_label']).round(2)\n",
    "result['normalized maximum'] = (result['maximum'] / result['da_label']).round(2)\n",
    "result['normalized range'] = (result['range'] / result['da_label']).round(2)\n",
    "result['normalized variance'] = (result['variance'] / result['da_label']).round(2)\n",
    "result['normalized standard deviation'] = (result['standard_deviation'] / result['da_label']).round(2)\n",
    "result['normalized percentile rank'] = (result['percentile_rank'] / result['da_label']).round(2)\n",
    "result['normalized quartile rank'] = (result['quartile_rank'] / result['da_label']).round(2)\n",
    "result['normalized boxplot'] = (result['boxplot'] / result['da_label']).round(2)\n",
    "#display(result)\n",
    "\n",
    "ax = result.loc[:, 'normalized count':'normalized boxplot'].plot(kind='bar', rot=0)\n",
    "ax.legend(bbox_to_anchor=(0.7, 0.3), labels=['Count', 'Percent', 'Mean', 'Median', 'Mode', 'Minimum', 'Maximum', 'Range', 'Variance', 'Standard deviation', 'Percentile rank', 'Quartile rank', 'Boxplot'])\n",
    "# make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Normalized statistical methods of descriptive statistics used for data analysis per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Proportion of statistical method used')\n",
    "plt.savefig('Figures/CQ7/des_stats_proportion_of_statistical_methods_used.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "figure, axis = plt.subplots(7, 2, figsize=(18, 22), sharey=True)\n",
    "# Plot barplots\n",
    "bar1 = sns.barplot(data = result, x = result.index, y='normalized count', ax=axis[0,0], color=sns.color_palette()[0])\n",
    "bar1.bar_label(bar1.containers[0])\n",
    "#make_axes_area_auto_adjustable(bar1)\n",
    "bar2 = sns.barplot(data = result, x = result.index, y='normalized percent', ax=axis[0,1], color=sns.color_palette()[1])\n",
    "bar2.bar_label(bar2.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar2)\n",
    "bar3 = sns.barplot(data = result, x = result.index, y='normalized mean', ax=axis[1,0], color=sns.color_palette()[2])\n",
    "bar3.bar_label(bar3.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar3)\n",
    "bar4 = sns.barplot(data = result, x = result.index, y='normalized median', ax=axis[1,1], color=sns.color_palette()[3])\n",
    "bar4.bar_label(bar4.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar4)\n",
    "bar5 = sns.barplot(data = result, x = result.index, y='normalized mode', ax=axis[2,0], color=sns.color_palette()[4])\n",
    "bar5.bar_label(bar5.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar5)\n",
    "bar6 = sns.barplot(data = result, x = result.index, y='normalized minimum', ax=axis[2,1], color=sns.color_palette()[5])\n",
    "bar6.bar_label(bar6.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar6)\n",
    "bar7 = sns.barplot(data = result, x = result.index, y='normalized maximum', ax=axis[3,0], color=sns.color_palette()[6])\n",
    "bar7.bar_label(bar7.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar7)\n",
    "bar8 = sns.barplot(data = result, x = result.index, y='normalized range', ax=axis[3,1], color=sns.color_palette()[7])\n",
    "bar8.bar_label(bar8.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar7)\n",
    "bar9 = sns.barplot(data = result, x = result.index, y='normalized variance', ax=axis[4,0], color=sns.color_palette()[8])\n",
    "bar9.bar_label(bar9.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar7)\n",
    "bar10 = sns.barplot(data = result, x = result.index, y='normalized standard deviation', ax=axis[4,1], color=sns.color_palette()[9])\n",
    "bar10.bar_label(bar10.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar7)\n",
    "bar11 = sns.barplot(data = result, x = result.index, y='normalized percentile rank', ax=axis[5,0], color=sns.color_palette()[0])\n",
    "bar11.bar_label(bar11.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar7)\n",
    "bar12 = sns.barplot(data = result, x = result.index, y='normalized quartile rank', ax=axis[5,1], color=sns.color_palette()[1])\n",
    "bar12.bar_label(bar12.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar7)\n",
    "bar13 = sns.barplot(data = result, x = result.index, y='normalized boxplot', ax=axis[6,0], color=sns.color_palette()[2])\n",
    "bar13.bar_label(bar13.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar7)\n",
    "\n",
    "# Remove unnecessary subplot\n",
    "figure.delaxes(axis[6,1])\n",
    "\n",
    "# Set titles\n",
    "plt.suptitle(\"Normalized number of statistical method used for data analysis per year grouped by statistical method\")\n",
    "bar1.set(title = 'Normalized number of count method used for data analysis per year', ylabel='Proportion of count method', xlabel='Year')\n",
    "bar2.set(title = 'Normalized number of percent method used for data analysis per year', ylabel='Proportion of percent method', xlabel='Year')\n",
    "bar3.set(title = 'Normalized number of mean method used for data analysis per year', ylabel='Proportion of mean method', xlabel='Year')\n",
    "bar4.set(title = 'Normalized number of median method used for data analysis per year', ylabel='Proportion of median method', xlabel='Year')\n",
    "bar5.set(title = 'Normalized number of mode method used for data analysis per year', ylabel='Proportion of mode method', xlabel='Year')\n",
    "bar6.set(title = 'Normalized number of minimum method used for data analyis per year', ylabel='Proportion of minimum method', xlabel='Year')\n",
    "bar7.set(title = 'Normalized number of maximum method used for data analysis per year', ylabel='Proportion of maximum method', xlabel='Year')\n",
    "bar8.set(title = 'Normalized number of range method used for data analysis per year', ylabel='Proportion of range method', xlabel='Year')\n",
    "bar9.set(title = 'Normalized number of variance method used for data analysis per year', ylabel='Proportion of variance method', xlabel='Year')\n",
    "bar10.set(title = 'Normalized number of standard deviation method used for data analysis per year', ylabel='Proportion of standard deviation method', xlabel='Year')\n",
    "bar11.set(title = 'Normalized number of percentile rank method used for data analysis per year', ylabel='Proportion of percentile rank method', xlabel='Year')\n",
    "bar12.set(title = 'Normalized number of quartile rank method used for data analysis per year', ylabel='Proportion of quartile rank method', xlabel='Year')\n",
    "bar13.set(title = 'Normalized number of boxplot method used for data analysis per year', ylabel='Proportion of boxplot method', xlabel='Year')\n",
    "\n",
    "# Set spacing between subplots\n",
    "figure.tight_layout()\n",
    "plt.savefig('Figures/CQ7/des_stats_proportion_of_statistical_methods_used_grouped_by_method.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data analysis, we consider the statistical methods of inferential statistics used for data analysis. We identify the statistical methods of inferenttial statistics used for data analysis. A paper can involve more than one statistical methods of inferential statistics used for data analysis so that the number of statistical methodsof inferential statistics used for data analysis can be larger than the number of papers using empricial methods for data analysis. We normalize the number of statistical methods of inferential statistics used for data analysis based on the number of all papers using at least one empirical method for data analysis.\n",
    "\n",
    "**%TODO\n",
    "HIER IST NOCH EIN PROBLEM! Die \"statistical tests\" sind als flexible Resource definiert. Aktuell gibt es 14 verschiedene Resourcen (Tests) er können aber theoretisch unendlich viele werden. Daher muss die Erstellung der Graphen pro Test langfristig in einer Schleife erfolgen statt der manuellen Erzeugung!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(df_query_7_2.groupby('year')['test'].value_counts().unstack())\n",
    "display(result)\n",
    "\n",
    "ax = result.plot(kind='bar', rot=0)\n",
    "ax.legend(bbox_to_anchor=(0.7, 0.3))\n",
    "# #make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Number of statistical methods of inferential statistics used for data analysis per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of statistical method used')\n",
    "plt.savefig('Figures/CQ7/inf_stats_number_of_statistical_methods_used.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "figure, axis = plt.subplots(7, 2, figsize=(18, 30), sharey=True)\n",
    "# Plot barplots\n",
    "bar1 = sns.barplot(data = result, x = result.index, y='fisher exact test', ax=axis[0,0], color=sns.color_palette()[0])\n",
    "bar1.bar_label(bar1.containers[0])\n",
    "#make_axes_area_auto_adjustable(bar1)\n",
    "bar2 = sns.barplot(data = result, x = result.index, y='spearman correlation', ax=axis[0,1], color=sns.color_palette()[1])\n",
    "bar2.bar_label(bar2.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar2)\n",
    "bar3 = sns.barplot(data = result, x = result.index, y='bonferroni adjustment', ax=axis[1,0], color=sns.color_palette()[2])\n",
    "bar3.bar_label(bar3.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar3)\n",
    "bar4 = sns.barplot(data = result, x = result.index, y='dunn-test', ax=axis[1,1], color=sns.color_palette()[3])\n",
    "bar4.bar_label(bar4.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar4)\n",
    "bar5 = sns.barplot(data = result, x = result.index, y='games-howell post-hoc', ax=axis[2,0], color=sns.color_palette()[4])\n",
    "bar5.bar_label(bar5.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar5)\n",
    "bar6 = sns.barplot(data = result, x = result.index, y='kruskal-wallis test', ax=axis[2,1], color=sns.color_palette()[5])\n",
    "bar6.bar_label(bar6.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar6)\n",
    "bar7 = sns.barplot(data = result, x = result.index, y='levene test', ax=axis[3,0], color=sns.color_palette()[6])\n",
    "bar7.bar_label(bar7.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar7)\n",
    "bar8 = sns.barplot(data = result, x = result.index, y='mann-whitney-u test', ax=axis[3,1], color=sns.color_palette()[7])\n",
    "bar8.bar_label(bar8.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar8)\n",
    "bar9 = sns.barplot(data = result, x = result.index, y='pearson correlation', ax=axis[4,0], color=sns.color_palette()[8])\n",
    "bar9.bar_label(bar9.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar9)\n",
    "bar10 = sns.barplot(data = result, x = result.index, y='shapiro-wilk test', ax=axis[4,1], color=sns.color_palette()[9])\n",
    "bar10.bar_label(bar10.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar10)\n",
    "bar11 = sns.barplot(data = result, x = result.index, y='t-test', ax=axis[5,0], color=sns.color_palette()[0])\n",
    "bar11.bar_label(bar11.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar11)\n",
    "bar12 = sns.barplot(data = result, x = result.index, y='welch t-test', ax=axis[5,1], color=sns.color_palette()[1])\n",
    "bar12.bar_label(bar12.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar12)\n",
    "bar13 = sns.barplot(data = result, x = result.index, y='wilcoxon signed rank test', ax=axis[6,0], color=sns.color_palette()[2])\n",
    "bar13.bar_label(bar13.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar13)\n",
    "\n",
    "# Remove unnecessary subplot\n",
    "figure.delaxes(axis[6,1])\n",
    "\n",
    "# Set titles\n",
    "plt.suptitle(\"Number of statistical method used for data analysis per year grouped by statistical method\", y=1)\n",
    "bar1.set(title = 'Number of fisher exact test used for data analysis per year', ylabel='Number of fisher exact tests', xlabel='Year')\n",
    "bar2.set(title = 'Number of spearman correlation used for data analysis per year', ylabel='Number of spearman correlations', xlabel='Year')\n",
    "bar3.set(title = 'Number of bonferroni adjustment used for data analysis per year', ylabel='Number of bonferroni adjustments', xlabel='Year')\n",
    "bar4.set(title = 'Number of dunn-test used for data analysis per year', ylabel='Number of dunn-tests', xlabel='Year')\n",
    "bar5.set(title = 'Number of games-howell post-hoc test used for data analysis per year', ylabel='Number of games-howell post-hoc tests', xlabel='Year')\n",
    "bar6.set(title = 'Number of kruskal-wallis test used for data analyis per year', ylabel='Number of kruskal-wallis tests', xlabel='Year')\n",
    "bar7.set(title = 'Number of levene test used for data analysis per year', ylabel='Number of levene tests', xlabel='Year')\n",
    "bar8.set(title = 'Number of mann-whitney-u test used for data analysis per year', ylabel='Number of mann-whitney-u tests', xlabel='Year')\n",
    "bar9.set(title = 'Number of pearson correlation used for data analysis per year', ylabel='Number of pearson correlations', xlabel='Year')\n",
    "bar10.set(title = 'Number of shapiro-wilk test used for data analysis per year', ylabel='Number of shapiro-wilk tests', xlabel='Year')\n",
    "bar11.set(title = 'Number of t-test used for data analysis per year', ylabel='Number of t-tests', xlabel='Year')\n",
    "bar12.set(title = 'Number of welch t-test method used for data analysis per year', ylabel='Number of welch t-tests', xlabel='Year')\n",
    "bar13.set(title = 'Number of wilcoxon signed rank test used for data analysis per year', ylabel='Number of wilcoxon signed rank tests', xlabel='Year')\n",
    "\n",
    "# Set spacing between subplots\n",
    "figure.tight_layout()\n",
    "plt.savefig('Figures/CQ7/inf_stats_number_of_statistical_methods_used_grouped_by_method.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "number_of_all_papers_with_da = df_query_7_1.groupby('year')['da_label'].count()\n",
    "\n",
    "#result = pd.concat([result, number_of_all_papers_with_da], axis=1)\n",
    "result['normalized fisher exact test'] = (result['fisher exact test'] / number_of_all_papers_with_da).round(3)\n",
    "result['normalized spearman correlation'] = (result['spearman correlation'] / number_of_all_papers_with_da).round(3)\n",
    "result['normalized bonferroni adjustment'] = (result['bonferroni adjustment'] / number_of_all_papers_with_da).round(3)\n",
    "result['normalized dunn-test'] = (result['dunn-test'] / number_of_all_papers_with_da).round(3)\n",
    "result['normalized games-howell post-hoc'] = (result['games-howell post-hoc'] / number_of_all_papers_with_da).round(3)\n",
    "result['normalized kruskal-wallis test'] = (result['kruskal-wallis test'] / number_of_all_papers_with_da).round(3)\n",
    "result['normalized levene test'] = (result['levene test'] / number_of_all_papers_with_da).round(3)\n",
    "result['normalized mann-whitney-u test'] = (result['mann-whitney-u test'] / number_of_all_papers_with_da).round(3)\n",
    "result['normalized pearson correlation'] = (result['pearson correlation'] / number_of_all_papers_with_da).round(3)\n",
    "result['normalized shapiro-wilk test'] = (result['shapiro-wilk test'] / number_of_all_papers_with_da).round(3)\n",
    "result['normalized t-test'] = (result['t-test'] / number_of_all_papers_with_da).round(3)\n",
    "result['normalized welch t-test'] = (result['welch t-test'] / number_of_all_papers_with_da).round(3)\n",
    "result['normalized wilcoxon signed rank test'] = (result['wilcoxon signed rank test'] / number_of_all_papers_with_da).round(3)\n",
    "#display(result)\n",
    "\n",
    "ax = result.loc[:, 'normalized fisher exact test':'normalized wilcoxon signed rank test'].plot(kind='bar', rot=0)\n",
    "ax.legend(bbox_to_anchor=(0.7, 0.3))\n",
    "# make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Normalized statistical methods of inferential statistics used for data analysis per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Proportion of statistical method used')\n",
    "plt.savefig('Figures/CQ7/inf_stats_proportion_of_statistical_methods_used.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "figure, axis = plt.subplots(7, 2, figsize=(18, 30), sharey=True)\n",
    "# Plot barplots\n",
    "bar1 = sns.barplot(data = result, x = result.index, y='normalized fisher exact test', ax=axis[0,0], color=sns.color_palette()[0])\n",
    "bar1.bar_label(bar1.containers[0])\n",
    "#make_axes_area_auto_adjustable(bar1)\n",
    "bar2 = sns.barplot(data = result, x = result.index, y='normalized spearman correlation', ax=axis[0,1], color=sns.color_palette()[1])\n",
    "bar2.bar_label(bar2.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar2)\n",
    "bar3 = sns.barplot(data = result, x = result.index, y='normalized bonferroni adjustment', ax=axis[1,0], color=sns.color_palette()[2])\n",
    "bar3.bar_label(bar3.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar3)\n",
    "bar4 = sns.barplot(data = result, x = result.index, y='normalized dunn-test', ax=axis[1,1], color=sns.color_palette()[3])\n",
    "bar4.bar_label(bar4.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar4)\n",
    "bar5 = sns.barplot(data = result, x = result.index, y='normalized games-howell post-hoc', ax=axis[2,0], color=sns.color_palette()[4])\n",
    "bar5.bar_label(bar5.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar5)\n",
    "bar6 = sns.barplot(data = result, x = result.index, y='normalized kruskal-wallis test', ax=axis[2,1], color=sns.color_palette()[5])\n",
    "bar6.bar_label(bar6.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar6)\n",
    "bar7 = sns.barplot(data = result, x = result.index, y='normalized levene test', ax=axis[3,0], color=sns.color_palette()[6])\n",
    "bar7.bar_label(bar7.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar7)\n",
    "bar8 = sns.barplot(data = result, x = result.index, y='normalized mann-whitney-u test', ax=axis[3,1], color=sns.color_palette()[7])\n",
    "bar8.bar_label(bar8.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar8)\n",
    "bar9 = sns.barplot(data = result, x = result.index, y='normalized pearson correlation', ax=axis[4,0], color=sns.color_palette()[8])\n",
    "bar9.bar_label(bar9.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar9)\n",
    "bar10 = sns.barplot(data = result, x = result.index, y='normalized shapiro-wilk test', ax=axis[4,1], color=sns.color_palette()[9])\n",
    "bar10.bar_label(bar10.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar10)\n",
    "bar11 = sns.barplot(data = result, x = result.index, y='normalized t-test', ax=axis[5,0], color=sns.color_palette()[0])\n",
    "bar11.bar_label(bar11.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar11)\n",
    "bar12 = sns.barplot(data = result, x = result.index, y='normalized welch t-test', ax=axis[5,1], color=sns.color_palette()[1])\n",
    "bar12.bar_label(bar12.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar12)\n",
    "bar13 = sns.barplot(data = result, x = result.index, y='normalized wilcoxon signed rank test', ax=axis[6,0], color=sns.color_palette()[2])\n",
    "bar13.bar_label(bar13.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar13)\n",
    "\n",
    "# Set titles\n",
    "plt.suptitle(\"Normalized number of statistical method used for data analysis per year grouped by statistical method\", y=1)\n",
    "bar1.set(title = 'Normalized number of fisher exact test used for data analysis per year', ylabel='Proportion of fisher exact tests', xlabel='Year')\n",
    "bar2.set(title = 'Normalized number of spearman correlation used for data analysis per year', ylabel='Proportion of spearman correlations', xlabel='Year')\n",
    "bar3.set(title = 'Normalized number of bonferroni adjustment used for data analysis per year', ylabel='Proportion of bonferroni adjustments', xlabel='Year')\n",
    "bar4.set(title = 'Normalized number of dunn-test used for data analysis per year', ylabel='Proportion of dunn-tests', xlabel='Year')\n",
    "bar5.set(title = 'Normalized number of games-howell post-hoc test used for data analysis per year', ylabel='Proportion of games-howell post-hoc tests', xlabel='Year')\n",
    "bar6.set(title = 'Normalized number of kruskal-wallis test used for data analyis per year', ylabel='Proportion of kruskal-wallis tests', xlabel='Year')\n",
    "bar7.set(title = 'Normalized number of levene test used for data analysis per year', ylabel='Proportion of levene tests', xlabel='Year')\n",
    "bar8.set(title = 'Normalized number of mann-whitney-u test used for data analysis per year', ylabel='Proportion of mann-whitney-u tests', xlabel='Year')\n",
    "bar9.set(title = 'Normalized number of pearson correlation used for data analysis per year', ylabel='Proportion of pearson correlations', xlabel='Year')\n",
    "bar10.set(title = 'Normalized number of shapiro-wilk test used for data analysis per year', ylabel='Proportion of shapiro-wilk tests', xlabel='Year')\n",
    "bar11.set(title = 'Normalized number of t-test used for data analysis per year', ylabel='Proportion of t-tests', xlabel='Year')\n",
    "bar12.set(title = 'Normalized number of welch t-test method used for data analysis per year', ylabel='Proportion of welch t-tests', xlabel='Year')\n",
    "bar13.set(title = 'Normalized number of wilcoxon signed rank test used for data analysis per year', ylabel='Proportion of wilcoxon signed rank tests', xlabel='Year')\n",
    "\n",
    "# Set spacing between subplots\n",
    "figure.tight_layout()\n",
    "plt.savefig('Figures/CQ7/inf_stats_proportion_of_statistical_methods_used_grouped_by_method.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Interpretation*</ins>\n",
    "Muss noch geschrieben werden, wenn die Daten alle geprüft sind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q8'></a>\n",
    "### 3.8 How has the reporting of threats to validity evolved over time?\n",
    "\n",
    "#### <ins>*Data Selection*</ins> \n",
    "\n",
    "*Explanation of the Competency Question*:\n",
    "\n",
    "According to [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30), the **\"current\" state of practice (2007)** shows that researchers **lack knwoledge about limitations and assumptions of statistical tests, power analysis and effect size estiamtion as well as do not well define populations**. For the **target state (2020 - 2025)**, [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30) envision that **all these aspects are sufficiently addressed**. All mentioned aspects related to threats to validity of an empirical study and this predicted change leads to the corresponding competency question.\n",
    "\n",
    "*Required Data for the Analysis*:\n",
    "\n",
    "We must retrieve all papers with their publication year that use our ORKG template and report on threats to validity.\n",
    "\n",
    "[Back to top](#step3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Collection*</ins> \n",
    "Below, we retrieve the required data from the developed body of knowledge of empirical research in RE from the [ORKG](https://www.orkg.org/orkg/) using its [SPARQL endpoint](https://orkg.org/sparql/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = '8'\n",
    "\n",
    "query_8 = \"\"\"\n",
    "        SELECT ?paper, ?year, ?threats, ?threats_label\n",
    "        WHERE {\n",
    "                ?paper orkgp:P31 ?contribution;\n",
    "                        orkgp:P29 ?year.\n",
    "                ?contribution a orkgc:C27001.\n",
    "\n",
    "                OPTIONAL{?contribution orkgp:P39099 ?threats.\n",
    "                        ?threats rdfs:label ?threats_label.\n",
    "                        }\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_8)\n",
    "\n",
    "df_query_8 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Exploration*</ins>\n",
    "Below, we explore the retrieved data, including its cleaning and validation, to prepare the data for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_8['threats_label'] = df_query_8['threats_label'].astype('category')\n",
    "display(df_query_8['threats_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Analysis*</ins>\n",
    "For this data analysis, we select all papers that report threats to validity. For more detailed insights, we normalize the number of all papers reporting threats to validity based on the number of all unique papers per year, as the total number of papers per year varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(df_query_8.drop_duplicates(subset=['paper']).reset_index(drop=True)['year'].value_counts())\n",
    "result.columns = ['number_of_all_papers']\n",
    "\n",
    "papers_with_threats = df_query_8[(df_query_8['threats_label'] != 'non')]\n",
    "papers_with_threats = papers_with_threats.drop_duplicates(subset=['paper'])\n",
    "result['number_of_papers_with_threats'] = papers_with_threats.reset_index(drop=True)['year'].value_counts()\n",
    "\n",
    "result['normalized_papers_with_threats'] = (result['number_of_papers_with_threats'] / result['number_of_all_papers']).round(2)\n",
    "#display(result.sort_index())\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.barplot(data=result, x=result.index, y='number_of_papers_with_threats', color='b')\n",
    "ax.bar_label(ax.containers[0])\n",
    "# make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Number of papers reporting threats to validity per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of papers reporting threats to validity')\n",
    "plt.savefig('Figures/CQ8/number_of_papers_with_threats.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.barplot(data=result, x=result.index, y='normalized_papers_with_threats', color='b')\n",
    "ax.bar_label(ax.containers[0], fmt='%.2f')\n",
    "# make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Normalized number of papers reporting threats to validity per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Proportion of papers reporting threats to validity')\n",
    "plt.savefig('Figures/CQ8/proportion_of_papers_with_threats.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Interpretation*</ins>\n",
    "Muss noch geschrieben werden, wenn die Daten alle geprüft sind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q9'></a>\n",
    "### 3.9 What types of threats to validity do the authors report?\n",
    "\n",
    "#### <ins>*Data Selection*</ins> \n",
    "\n",
    "*Explanation of the Competency Question*:\n",
    "\n",
    "According to [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30), the **\"current\" state of practice (2007)** shows that researchers **lack knwoledge about limitations and assumptions of statistical tests, power analysis and effect size estiamtion as well as do not well define populations**. For the **target state (2020 - 2025)**, [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30) envision that **all these aspects are sufficiently addressed**. All mentioned aspects related to threats to validity of an empirical study and this predicted change leads to the corresponding competency question.\n",
    "\n",
    "*Required Data for the Analysis*:\n",
    "\n",
    "We must retrieve all papers with their publication year that use our ORKG template and report on threats to validity.\n",
    "\n",
    "[Back to top](#step3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Collection*</ins> \n",
    "Below, we retrieve the required data from the developed body of knowledge of empirical research in RE from the [ORKG](https://www.orkg.org/orkg/) using its [SPARQL endpoint](https://orkg.org/sparql/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = '9'\n",
    "\n",
    "query_9 = \"\"\"\n",
    "        SELECT ?paper, ?year, ?threats_label, ?external, ?internal, ?construct, ?conclusion, ?reliability, ?generalizability, ?content, ?descriptive, ?theoretical, ?repeatability\n",
    "        WHERE {\n",
    "                ?paper orkgp:P31 ?contribution;\n",
    "                        orkgp:P29 ?year.\n",
    "                ?contribution a orkgc:C27001.\n",
    "\n",
    "                OPTIONAL{?contribution orkgp:P39099 ?threats.\n",
    "                        ?threats rdfs:label ?threats_label.\n",
    "                        OPTIONAL{?threats orkgp:P55034 ?external.}\n",
    "                        OPTIONAL{?threats orkgp:P55035 ?internal.}\n",
    "                        OPTIONAL{?threats orkgp:P55037 ?construct.}\n",
    "                        OPTIONAL{?threats orkgp:P55036 ?conclusion.}\n",
    "                        OPTIONAL{?threats orkgp:P59109 ?reliability.}\n",
    "                        OPTIONAL{?threats orkgp:P60006 ?generalizability.}\n",
    "                        OPTIONAL{?threats orkgp:P68005 ?content.}\n",
    "                        OPTIONAL{?threats orkgp:P97000 ?descriptive.}\n",
    "                        OPTIONAL{?threats orkgp:P97001 ?theoretical.}\n",
    "                        OPTIONAL{?threats orkgp:P97002 ?repeatability.} \n",
    "                        }\n",
    "                }\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_9)\n",
    "\n",
    "df_query_9 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Exploration*</ins>\n",
    "Below, we explore the retrieved data, including its cleaning and validation, to prepare the data for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_9['external'] = df_query_9['external'].fillna(False).astype(bool)\n",
    "df_query_9['internal'] = df_query_9['internal'].fillna(False).astype(bool)\n",
    "df_query_9['construct'] = df_query_9['construct'].fillna(False).astype(bool)\n",
    "df_query_9['conclusion'] = df_query_9['conclusion'].fillna(False).astype(bool)\n",
    "df_query_9['reliability'] = df_query_9['reliability'].fillna(False).astype(bool)\n",
    "df_query_9['generalizability'] = df_query_9['generalizability'].fillna(False).astype(bool)\n",
    "df_query_9['content'] = df_query_9['content'].fillna(False).astype(bool)\n",
    "df_query_9['descriptive'] = df_query_9['descriptive'].fillna(False).astype(bool)\n",
    "df_query_9['theoretical'] = df_query_9['theoretical'].fillna(False).astype(bool)\n",
    "df_query_9['repeatability'] = df_query_9['repeatability'].fillna(False).astype(bool)\n",
    "#display(df_query_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Analysis*</ins>\n",
    "For this data analysis, we select all papers that report threats to validity. For more detailed insights, we normalize the number of all papers reporting threats to validity based on the number of all unique papers per year, as the total number of papers per year varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(df_query_9[['external', 'internal','construct', 'conclusion', 'reliability', 'generalizability', 'content', 'descriptive', 'theoretical', 'repeatability']].sum().sort_values())\n",
    "result.columns = ['number_of_papers_with_threats']\n",
    "result = result.rename(index={'external': 'External validity', 'internal': 'Internal validity','construct': 'Construct validity', 'conclusion': 'Conclusion validity', 'reliability': 'Reliability', 'generalizability': 'Generalizability', 'content': 'Content validity', 'descriptive': 'Descriptive validity', 'theoretical': 'Theoretical validity', 'repeatability': 'Repeatability'})\n",
    "number_of_papers_with_threats_mentioned = df_query_9[df_query_9['threats_label'] == 'mentioned']['threats_label'].count()\n",
    "mentioned = pd.DataFrame([number_of_papers_with_threats_mentioned], index=['Mentioned, but not classified'], columns=['number_of_papers_with_threats'])\n",
    "result = pd.concat([mentioned, result])\n",
    "#display(result)\n",
    "\n",
    "ax = result.plot(kind='barh', rot=0)\n",
    "#ax.invert_yaxis()\n",
    "ax.bar_label(ax.containers[0])\n",
    "ax.get_legend().remove()\n",
    "make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Number of threats to validity reported in papers')\n",
    "plt.xlabel('Number of threats to validity reported')\n",
    "plt.ylabel('Threats to validity reported')\n",
    "plt.savefig('Figures/CQ9/number_of_threats_to_validity_reported.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "number_of_papers_with_threats = df_query_9[(df_query_9['threats_label'] != 'non')]['paper'].count()\n",
    "result['normalized'] = (result['number_of_papers_with_threats'] / number_of_papers_with_threats).round(3)\n",
    "#display(result)\n",
    "\n",
    "plt.figure()\n",
    "ax = result.loc[:, 'normalized'].plot(kind='barh', rot=0)\n",
    "ax.bar_label(ax.containers[0])\n",
    "make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Normalized number of threats to validity reported in papers')\n",
    "plt.xlabel('Proportion of threats to validity reported')\n",
    "plt.ylabel('Threats to validity reported')\n",
    "plt.savefig('Figures/CQ9/proportion_of_threats_to_validity_reported.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Interpretation*</ins>\n",
    "Muss noch geschrieben werden, wenn die Daten alle geprüft sind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q10'></a>\n",
    "### 3.10 How have the proportions of case studies and action research in the empirical methods used evolved over time?\n",
    "\n",
    "#### <ins>*Data Selection*</ins> \n",
    "\n",
    "*Explanation of the Competency Question*:\n",
    "\n",
    "According to [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30), one may question the **industrial relevance of most the studies** in the **\"current\" state of practice (2007)**. For the **target state (2020 - 2025)**, [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30) envision that researchers carry out **more case studies and actions research**. This predicted change leads to the corresponding competency question.\n",
    "\n",
    "*Required Data for the Analysis*:\n",
    "\n",
    "We must retrieve all papers with their publication year that use our ORKG template and report on the use of the empirical methods: case studies and action research.\n",
    "\n",
    "[Back to top](#step3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Collection*</ins> \n",
    "Below, we retrieve the required data from the developed body of knowledge of empirical research in RE from the [ORKG](https://www.orkg.org/orkg/) using its [SPARQL endpoint](https://orkg.org/sparql/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = '10'\n",
    "\n",
    "query_10 = \"\"\"\n",
    "        SELECT ?paper, ?year, ?dc_label\n",
    "        WHERE {\n",
    "                ?paper orkgp:P31 ?contribution;\n",
    "                        orkgp:P29 ?year.\n",
    "                ?contribution a orkgc:C27001.\n",
    "\n",
    "                OPTIONAL{?contribution orkgp:P56008 ?data_collection.\n",
    "                        ?data_collection rdfs:label ?dc_label.\n",
    "                }\n",
    "                FILTER(?dc_label != \"no collection\"^^xsd:string)\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_10)\n",
    "df_query_10 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Exploration*</ins>\n",
    "Below, we explore the retrieved data, including its cleaning and validation, to prepare the data for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_10['dc_label'] = df_query_10['dc_label'].astype('category')\n",
    "display(df_query_10['dc_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Analysis*</ins>\n",
    "For this data analysis, we consider all papers that report on the use of the empirical methods: case study and action research. A paper can involve more than one empirical method for data collection so that the number of empirical methods can be larger than the number of papers. We normalize the number of empirical methods (case study and action research) used based on the number of all unique papers with data collection per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers_per_year = pd.DataFrame(df_query_10.drop_duplicates(subset=['paper']).reset_index(drop=True)['year'].value_counts())\n",
    "all_papers_per_year.columns = ['number_of_papers_with_dc']\n",
    "all_papers_per_year = all_papers_per_year.sort_index()\n",
    "\n",
    "result = pd.DataFrame(df_query_10.groupby('year')['dc_label'].value_counts().unstack())\n",
    "result = result[result.sum().sort_values(ascending=False).index]\n",
    "result = result[['case study', 'action research']]\n",
    "#display(result)\n",
    "\n",
    "result = pd.concat([result, all_papers_per_year], axis=1)\n",
    "result['normalized case study'] = (result['case study'] / result['number_of_papers_with_dc']).round(2)\n",
    "result['normalized action research'] = (result['action research'] / result['number_of_papers_with_dc']).round(2)\n",
    "#display(result)\n",
    "\n",
    "ax = result.loc[:, 'normalized case study':'normalized action research'].plot(kind='bar', rot=0)\n",
    "ax.legend(bbox_to_anchor=(0.8, 0.8), labels=['Case study', 'Action research',])\n",
    "# make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Normalized number of case studies and action research used for data collection per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Proportion of empirical method used')\n",
    "plt.savefig('Figures/CQ10/proportion_of_case_studies_and_action_research.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "figure, axis = plt.subplots(1, 2, figsize=(18, 8), sharey=True)\n",
    "# Plot barplots\n",
    "bar1 = sns.barplot(data = result, x = result.index, y='normalized case study', ax=axis[0], color=sns.color_palette()[0])\n",
    "bar1.bar_label(bar1.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar1)\n",
    "bar2 = sns.barplot(data = result, x = result.index, y='normalized action research', ax=axis[1], color=sns.color_palette()[1])\n",
    "bar2.bar_label(bar2.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar2)\n",
    "\n",
    "# # Set titles\n",
    "plt.suptitle(\"Normalized number of case studies and action research used for data collection per year grouped by empirical method\")\n",
    "bar1.set(title = 'Normalized number of case studies used for data collection per year', ylabel='Proportion of case studies', xlabel='Year')\n",
    "bar2.set(title = 'Normalized number of action research used for data collection per year', ylabel='Proportion of action research', xlabel='Year')\n",
    "\n",
    "# # Set spacing between subplots\n",
    "figure.tight_layout()\n",
    "plt.savefig('Figures/CQ10/proportion_of_case_studies_and_action_research_grouped_by_method.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Interpretation*</ins>\n",
    "Muss noch geschrieben werden, wenn die Daten alle geprüft sind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q11'></a>\n",
    "### 3.11 How has the provision of data (the materials used, raw data collected, and study results identified) evolved over time?\n",
    "\n",
    "#### <ins>*Data Selection*</ins> \n",
    "\n",
    "*Explanation of the Competency Question*:\n",
    "\n",
    "According to [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30), the **\"current\" state of practice (2007)** shows that **few studies** provide results, materials, and raw data that enable efficient cumulative research. For the **target state (2020 - 2025)**, [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30) envision that **more research studies** are designed with the goal of enabling efficient use of its results by other researchers. This predicted change leads to the corresponding competency question.\n",
    "\n",
    "*Required Data for the Analysis*:\n",
    "\n",
    "We must retrieve all papers with their publication year that use our ORKG template and provide at least one url for the availiability of the materials used, raw data collected, and study results.\n",
    "\n",
    "[Back to top](#step3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Collection*</ins> \n",
    "Below, we retrieve the required data from the developed body of knowledge of empirical research in RE from the [ORKG](https://www.orkg.org/orkg/) using its [SPARQL endpoint](https://orkg.org/sparql/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = '11'\n",
    "\n",
    "query_11 = \"\"\"\n",
    "        SELECT ?paper, ?year, ?data, ?url\n",
    "        WHERE {\n",
    "                ?paper orkgp:P31 ?contribution;\n",
    "                        orkgp:P29 ?year.\n",
    "                ?contribution a orkgc:C27001.\n",
    "\n",
    "                OPTIONAL{?contribution orkgp:P56008 ?data_collection.\n",
    "                        ?data_collection rdfs:label ?dc_label.\n",
    "                        OPTIONAL{?data_collection orkgp:DATA ?data.\n",
    "                                OPTIONAL{?data orkgp:url ?url.}\n",
    "                                }\n",
    "                        }\n",
    "                FILTER(?dc_label != \"no collection\"^^xsd:string)\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_11)\n",
    "df_query_11 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Exploration*</ins>\n",
    "Below, we explore the retrieved data, including its cleaning and validation, to prepare the data for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Analysis*</ins>\n",
    "For this data analysis, we consider all papers that provide at least one url for the availiability of the materials used, raw data collected, and study results. A paper can involve more than one url for the data. For this reason, we determine the number of papers that provide at least one url and we normalize the number of papers with at least one url based on the number of all unique papers with data collection per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers_per_year = pd.DataFrame(df_query_11.drop_duplicates(subset=['paper']).reset_index(drop=True)['year'].value_counts())\n",
    "all_papers_per_year.columns = ['number_of_papers_with_dc']\n",
    "all_papers_per_year = all_papers_per_year.sort_index()\n",
    "\n",
    "result = pd.DataFrame(df_query_11.drop_duplicates(subset=['paper']).groupby('year')['url'].count())\n",
    "result = result[result.sum().sort_values(ascending=False).index]\n",
    "#display(result)\n",
    "\n",
    "ax = result.plot(kind='bar', rot=0)\n",
    "ax.get_legend().remove()\n",
    "# make_axes_area_auto_adjustable(ax)\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.title('Number of papers that provide at least one URL to data per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of papers with data')\n",
    "plt.savefig('Figures/CQ11/number_of_paper_with_data.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "result = pd.concat([result, all_papers_per_year], axis=1)\n",
    "result['normalized'] = (result['url'] / result['number_of_papers_with_dc']).round(2)\n",
    "#display(result)\n",
    "\n",
    "plt.figure()\n",
    "ax = result['normalized'].plot(kind='bar', rot=0)\n",
    "# ax.legend(bbox_to_anchor=(0.8, 0.8))\n",
    "# # make_axes_area_auto_adjustable(ax)\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.title('Normalized number of papers that provide at least one URL to data per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Proportion of papers with data')\n",
    "plt.savefig('Figures/CQ11/proportion_of_paper_with_data.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Interpretation*</ins>\n",
    "Muss noch geschrieben werden, wenn die Daten alle geprüft sind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q12'></a>\n",
    "### 3.12 How has the reporting of research questions and answers evolved over time?\n",
    "\n",
    "#### <ins>*Data Selection*</ins> \n",
    "\n",
    "*Explanation of the Competency Question*:\n",
    "\n",
    "According to [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30), the **\"current\" state of practice (2007)** shows that **results are hidden**. For the **target state (2020 - 2025)**, [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30) envision that researchers **focus more on communicating important results**, such as the research question and its answer. This predicted change leads to the corresponding competency question.\n",
    "\n",
    "*Required Data for the Analysis*:\n",
    "\n",
    "We must retrieve all papers with their publication year that use our ORKG template and have information about the research questions and answer.\n",
    "\n",
    "[Back to top](#step3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Collection*</ins> \n",
    "Below, we retrieve the required data from the developed body of knowledge of empirical research in RE from the [ORKG](https://www.orkg.org/orkg/) using its [SPARQL endpoint](https://orkg.org/sparql/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = '12'\n",
    "\n",
    "query_12 = \"\"\"\n",
    "        SELECT ?paper, ?year, ?question, ?highlighted_q, ?highlighted_a\n",
    "WHERE {\n",
    "  ?paper orkgp:P31 ?contribution;\n",
    "         orkgp:P29 ?year.\n",
    "  ?contribution a orkgc:C27001.\n",
    "\n",
    "  OPTIONAL{?contribution orkgp:P37330 ?rq.\n",
    "           OPTIONAL{?rq orkgp:P44139 ?question.}\n",
    "           OPTIONAL{?rq orkgp:P55039 ?highlighted_q.}\n",
    "          }\n",
    "  OPTIONAL{?contribution orkgp:P57004 ?answer.\n",
    "           OPTIONAL{?answer orkgp:P55039 ?highlighted_a.}\n",
    "          }\n",
    "}\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_12)\n",
    "df_query_12 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Exploration*</ins>\n",
    "Below, we explore the retrieved data, including its cleaning and validation, to prepare the data for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Analysis*</ins>\n",
    "For this data analysis, we consider all papers that report at least one research question (either highlighted or hidden). We also assume that all papers provide an answer (highlighted or hidden) in some way as they are published research paper. A paper can involve more than one research question so that the number of research questions can be larger than the number of papers. We normalize the number of papers with hightlighted/hidden research question(s) and anwers based on the number of all unique papers per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_per_year = df_query_12.drop_duplicates(subset=['paper']).reset_index(drop=True).groupby('year')['paper'].count()\n",
    "#display(papers_per_year)\n",
    "\n",
    "papers_with_rq = df_query_12.dropna()\n",
    "\n",
    "high_q_high_a = pd.DataFrame(papers_with_rq.loc[(papers_with_rq['highlighted_q'] == True) & (papers_with_rq['highlighted_a'] == True)].drop_duplicates(subset=['paper']).reset_index(drop=True).groupby('year')['paper'].count())\n",
    "high_q_high_a.columns = ['number_of_papers']\n",
    "high_q_hid_a = pd.DataFrame(papers_with_rq.loc[(papers_with_rq['highlighted_q'] == True) & (papers_with_rq['highlighted_a'] == False)].drop_duplicates(subset=['paper']).reset_index(drop=True).groupby('year')['paper'].count())\n",
    "high_q_hid_a.columns = ['number_of_papers']\n",
    "#At the moment there is no entry with a hidden RQ and a highlighted answer so that the table is empty and cannot be visualized\n",
    "#hid_q_high_a = pd.DataFrame(papers_with_rq.loc[(papers_with_rq['highlighted_q'] == False) & (papers_with_rq['highlighted_a'] == True)].drop_duplicates(subset=['paper']).reset_index(drop=True).groupby('year')['paper'].count())\n",
    "#hid_q_high_a.columns = ['number_of_papers']\n",
    "hid_q_hid_a = pd.DataFrame(papers_with_rq.loc[(papers_with_rq['highlighted_q'] == False) & (papers_with_rq['highlighted_a'] == False)].drop_duplicates(subset=['paper']).reset_index(drop=True).groupby('year')['paper'].count())\n",
    "hid_q_hid_a.columns = ['number_of_papers']\n",
    "\n",
    "\n",
    "figure, axis = plt.subplots(2, 2, figsize=(18, 10), sharey=True)\n",
    "# Plot barplots\n",
    "bar1 = sns.barplot(data = high_q_high_a, x = high_q_high_a.index, y='number_of_papers', ax=axis[0,0], color=sns.color_palette()[0])\n",
    "bar1.bar_label(bar1.containers[0])\n",
    " #make_axes_area_auto_adjustable(bar1)\n",
    "bar2 = sns.barplot(data = high_q_hid_a, x = high_q_hid_a.index, y='number_of_papers', ax=axis[0,1], color=sns.color_palette()[1])\n",
    "bar2.bar_label(bar2.containers[0])\n",
    "# # make_axes_area_auto_adjustable(bar2)\n",
    "#bar3 = sns.barplot(data = hid_q_high_a, x = hid_q_high_a.index, y='number_of_papers', ax=axis[1,0], color=sns.color_palette()[2])\n",
    "#bar3.bar_label(bar3.containers[0])\n",
    "# # make_axes_area_auto_adjustable(bar3)\n",
    "bar4 = sns.barplot(data = hid_q_hid_a, x = hid_q_hid_a.index, y='number_of_papers', ax=axis[1,1], color=sns.color_palette()[3])\n",
    "bar4.bar_label(bar4.containers[0])\n",
    "# # make_axes_area_auto_adjustable(bar4)\n",
    "\n",
    "# # Set titles\n",
    "plt.suptitle(\"Number of papers highlighting/hiding research question(s) and answers per year grouped by highlighting/hiding combination\")\n",
    "bar1.set(title = 'Number of papers with highlighted research question(s) and highlighted answers per year', ylabel='Number of papers', xlabel='Year')\n",
    "bar2.set(title = 'Number of papers with highlighted research question(s) and hidden answers per year', ylabel='Number of papers', xlabel='Year')\n",
    "#bar3.set(title = 'Number of papers with hidden research question(s) and highlighted answers per year', ylabel='Number of papers', xlabel='Year')\n",
    "bar4.set(title = 'Number of papers with hidden research questions(s) and hidden answers per year', ylabel='Number of papers', xlabel='Year')\n",
    "\n",
    "# # Set spacing between subplots\n",
    "figure.tight_layout()\n",
    "plt.savefig('Figures/CQ12/number_of_papers_high_hide_rq_a.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "high_q_high_a['normalized'] = (high_q_high_a['number_of_papers'] / papers_per_year).round(2)\n",
    "high_q_hid_a['normalized'] = (high_q_hid_a['number_of_papers'] / papers_per_year).round(2)\n",
    "hid_q_high_a['normalized'] = (hid_q_high_a['number_of_papers'] / papers_per_year).round(2)\n",
    "hid_q_hid_a['normalized'] = (hid_q_hid_a['number_of_papers'] / papers_per_year).round(2)\n",
    "\n",
    "figure, axis = plt.subplots(2, 2, figsize=(18, 10), sharey=True)\n",
    "# Plot barplots\n",
    "bar1 = sns.barplot(data = high_q_high_a, x = high_q_high_a.index, y='normalized', ax=axis[0,0], color=sns.color_palette()[0])\n",
    "bar1.bar_label(bar1.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar1)\n",
    "bar2 = sns.barplot(data = high_q_hid_a, x = high_q_hid_a.index, y='normalized', ax=axis[0,1], color=sns.color_palette()[1])\n",
    "bar2.bar_label(bar2.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar2)\n",
    "bar3 = sns.barplot(data = hid_q_high_a, x = hid_q_high_a.index, y='normalized', ax=axis[1,0], color=sns.color_palette()[2])\n",
    "bar3.bar_label(bar3.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar3)\n",
    "bar4 = sns.barplot(data = hid_q_hid_a, x = hid_q_hid_a.index, y='normalized', ax=axis[1,1], color=sns.color_palette()[3])\n",
    "bar4.bar_label(bar4.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar4)\n",
    "\n",
    "# # Set titles\n",
    "plt.suptitle(\"Normalized number of papers highlighting/hiding research question(s) and answers per year grouped by highlighting/hiding combination\")\n",
    "bar1.set(title = 'Proportion of papers with highlighted research question(s) and highlighted answers per year', ylabel='Proportion of papers', xlabel='Year')\n",
    "bar2.set(title = 'Proportion of papers with highlighted research question(s) and hidden answers per year', ylabel='Proportion of papers', xlabel='Year')\n",
    "bar3.set(title = 'Proportion of papers with hidden research question(s) and highlighted answers per year', ylabel='Proportion of papers', xlabel='Year')\n",
    "bar4.set(title = 'Proportion of papers with hidden research questions(s) and hidden answers per year', ylabel='Proportion of papers', xlabel='Year')\n",
    "\n",
    "# # Set spacing between subplots\n",
    "figure.tight_layout()\n",
    "plt.savefig('Figures/CQ12/proportion_of_papers_high_hide_rq_a.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis, we consider all papers that do not report any research question. We also assume that even the papers without a research question provide an answer (highlighted or hidden) in some way as they are published research papers. We normalize the number of papers without any research question and highlighted/hidden answers based on the number of all unique papers per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_with_no_rq = df_query_12[df_query_12['question'].isnull()].reset_index(drop=True)\n",
    "no_rq_high_a = pd.DataFrame(papers_with_no_rq.loc[papers_with_no_rq['highlighted_a'] == True].drop_duplicates(subset=['paper']).reset_index(drop=True).groupby('year')['paper'].count())\n",
    "no_rq_high_a.columns = ['number_of_papers']\n",
    "no_rq_hid_a = pd.DataFrame(papers_with_no_rq.loc[(papers_with_no_rq['highlighted_a'] == False)].drop_duplicates(subset=['paper']).reset_index(drop=True).groupby('year')['paper'].count())\n",
    "no_rq_hid_a.columns = ['number_of_papers']\n",
    "#display(no_rq_high_a)\n",
    "#display(no_rq_hid_a)\n",
    "\n",
    "no_rq_high_a['normalized'] = (no_rq_high_a['number_of_papers'] / papers_per_year).round(2)\n",
    "no_rq_hid_a['normalized'] = (no_rq_hid_a['number_of_papers'] / papers_per_year).round(2)\n",
    "\n",
    "no_rq_high_a = no_rq_high_a.fillna(0)\n",
    "\n",
    "# Plot barplots\n",
    "figure, axis = plt.subplots(1, 2, figsize=(18, 5), sharey=True)\n",
    "bar1 = sns.barplot(data = no_rq_high_a, x = no_rq_high_a.index, y='number_of_papers', ax=axis[0], color=sns.color_palette()[0])\n",
    "bar1.bar_label(bar1.containers[0])\n",
    "#make_axes_area_auto_adjustable(bar1)\n",
    "\n",
    "bar2 = sns.barplot(data = no_rq_hid_a, x = no_rq_hid_a.index, y='number_of_papers', ax=axis[1], color=sns.color_palette()[1])\n",
    "bar2.bar_label(bar2.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar2)\n",
    "\n",
    "# Set titles\n",
    "plt.suptitle(\"Number of papers without research question(s) and highlighted/hidden answers per year grouped by highlighed/hidden answer\")\n",
    "bar1.set(title = 'Number of papers without research question and highlighted answers per year', ylabel='Number of papers', xlabel='Year')\n",
    "bar2.set(title = 'Number of papers without research question and hidden answers per year', ylabel='Number of papers', xlabel='Year')\n",
    "\n",
    "# Set spacing between subplots\n",
    "figure.tight_layout()\n",
    "plt.savefig('Figures/CQ12/number_of_papers_without_rq_but_a.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "figure, axis = plt.subplots(1, 2, figsize=(18, 5), sharey=True)\n",
    "# Plot barplots\n",
    "bar1 = sns.barplot(data = no_rq_high_a, x = no_rq_high_a.index, y='normalized', ax=axis[0], color=sns.color_palette()[0])\n",
    "bar1.bar_label(bar1.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar1)\n",
    "bar2 = sns.barplot(data = no_rq_hid_a, x = no_rq_hid_a.index, y='normalized', ax=axis[1], color=sns.color_palette()[1])\n",
    "bar2.bar_label(bar2.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar2)\n",
    "\n",
    "# Set titles\n",
    "plt.suptitle(\"Normlaized number of papers without research question(s) and highlighted/hidden answers per year grouped by highlighed/hidden answer\")\n",
    "bar1.set(title = 'Proportion of papers without research question and highlighted answers per year', ylabel='Proportion of papers', xlabel='Year')\n",
    "bar2.set(title = 'Proportion of papers without research question and hidden answers per year', ylabel='Proportion of papers', xlabel='Year')\n",
    "\n",
    "# Set spacing between subplots\n",
    "figure.tight_layout()\n",
    "plt.savefig('Figures/CQ12/proportion_of_papers_without_rq_but_a.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Interpretation*</ins>\n",
    "Muss noch geschrieben werden, wenn die Daten alle geprüft sind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q13'></a>\n",
    "### 3.13 What empirical methods are used to conduct integrative and interpretive (systematic literature) reviews, so-called secondary research?\n",
    "\n",
    "#### <ins>*Data Selection*</ins> \n",
    "\n",
    "*Explanation of the Competency Question*:\n",
    "\n",
    "According to [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30), the **\"current\" state of practice (2007)** shows (literature) **reviews**, so-called secondary research, are mainly **narrative and biased**, and researchers have **little appreciation of the value of systematic (literature) reviews**. For the **target state (2020 - 2025)**, [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30) envision that **scientific methods are used to undertake integrative and interpretive (literature) reviews**. This predicted change leads to the corresponding competency question.\n",
    "\n",
    "*Required Data for the Analysis*:\n",
    "\n",
    "We must retrieve all papers that use our ORKG template and secondardy research as the empirical method for data collection.\n",
    "\n",
    "[Back to top](#step3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Collection*</ins> \n",
    "Below, we retrieve the required data from the developed body of knowledge of empirical research in RE from the [ORKG](https://www.orkg.org/orkg/) using its [SPARQL endpoint](https://orkg.org/sparql/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = '13'\n",
    "\n",
    "query_13 = \"\"\"\n",
    "        SELECT ?paper, ?dc_label, ?method_label\n",
    "        WHERE {\n",
    "                ?paper orkgp:P31 ?contribution.\n",
    "                ?contribution a orkgc:C27001.\n",
    "\n",
    "                ?contribution orkgp:P56008 ?data_collection.\n",
    "                ?data_collection rdfs:label ?dc_label;\n",
    "                                orkgp:P57021 ?method.\n",
    "                ?method rdfs:label ?method_label.\n",
    "                \n",
    "                FILTER(?dc_label = \"secondary research\"^^xsd:string)\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_13)\n",
    "df_query_13 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Exploration*</ins>\n",
    "Below, we explore the retrieved data, including its cleaning and validation, to prepare the data for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_13['dc_label'] = df_query_13['dc_label'].astype('category')\n",
    "df_query_13['method_label'] = df_query_13['method_label'].astype('category')\n",
    "display(df_query_13['dc_label'].value_counts())\n",
    "display(df_query_13['method_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Analysis*</ins>\n",
    "For this data analysis, we consider all papers that that use our ORKG template and secondardy research as the empirical method for data collection. From these papers, we extract the empirical methods used for secondary research. We normalize the number of papers using empirical mehtods for secondary research based on the number of all papers that use secondary research for data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(df_query_13['method_label'].value_counts())\n",
    "result.index = result.index.map(str.capitalize)\n",
    "#display(result)\n",
    "\n",
    "ax = result.plot(kind='barh', rot=0)\n",
    "ax.invert_yaxis()\n",
    "ax.bar_label(ax.containers[0])\n",
    "ax.get_legend().remove()\n",
    "make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Number of empirical methods used for secondary research')\n",
    "plt.xlabel('Number of empirical method used')\n",
    "plt.ylabel('Empirical method used')\n",
    "plt.savefig('Figures/CQ13/number_of_empirical_methods_used_for_secondary_research.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "number_of_all_papers = result['method_label'].sum()\n",
    "result['normalized'] = (result['method_label'] / number_of_all_papers).round(2)\n",
    "#display(result)\n",
    "\n",
    "plt.figure()\n",
    "ax = result.loc[:, 'normalized'].plot(kind='barh', rot=0)\n",
    "ax.invert_yaxis()\n",
    "ax.bar_label(ax.containers[0])\n",
    "make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Normalized number of empirical methods used for secondary research')\n",
    "plt.xlabel('Proportion of empirical method used')\n",
    "plt.ylabel('Empirical method used')\n",
    "plt.savefig('Figures/CQ13/proportion_of_empirical_methods_used_for_secondary_research.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Interpretation*</ins>\n",
    "Muss noch geschrieben werden, wenn die Daten alle geprüft sind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q14'></a>\n",
    "### 3.14 How has the proportions of empirical methods to conduct (systematic literature) reviews, so-called secondary research, evolved over time?\n",
    "\n",
    "#### <ins>*Data Selection*</ins> \n",
    "\n",
    "*Explanation of the Competency Question*:\n",
    "\n",
    "According to [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30), the **\"current\" state of practice (2007)** shows (literature) **reviews**, so-called secondary research, are mainly **narrative and biased**, and researchers have **little appreciation of the value of systematic (literature) reviews**. For the **target state (2020 - 2025)**, [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30) envision that **scientific methods are used to undertake integrative and interpretive (literature) reviews**. This predicted change leads to the corresponding competency question.\n",
    "\n",
    "*Required Data for the Analysis*:\n",
    "\n",
    "We must retrieve all papers with their publication year that use our ORKG template and secondardy research as the empirical method for data collection.\n",
    "\n",
    "[Back to top](#step3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Collection*</ins> \n",
    "Below, we retrieve the required data from the developed body of knowledge of empirical research in RE from the [ORKG](https://www.orkg.org/orkg/) using its [SPARQL endpoint](https://orkg.org/sparql/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = '14'\n",
    "\n",
    "query_14 = \"\"\"\n",
    "        SELECT ?paper, ?year, ?dc_label, ?method_label\n",
    "        WHERE {\n",
    "                ?paper orkgp:P31 ?contribution;\n",
    "                        orkgp:P29 ?year.\n",
    "                ?contribution a orkgc:C27001.\n",
    "\n",
    "                ?contribution orkgp:P56008 ?data_collection.\n",
    "                ?data_collection rdfs:label ?dc_label;\n",
    "                                orkgp:P57021 ?method.\n",
    "                ?method rdfs:label ?method_label.\n",
    "                \n",
    "                FILTER(?dc_label = \"secondary research\"^^xsd:string)\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_14)\n",
    "df_query_14 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Exploration*</ins>\n",
    "Below, we explore the retrieved data, including its cleaning and validation, to prepare the data for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_14['dc_label'] = df_query_14['dc_label'].astype('category')\n",
    "df_query_14['method_label'] = df_query_14['method_label'].astype('category')\n",
    "display(df_query_14['dc_label'].value_counts())\n",
    "display(df_query_14['method_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Analysis*</ins>\n",
    "For this data analysis, we consider all papers that that use our ORKG template and secondardy research as the empirical method for data collection. From these papers, we extract the empirical methods used for secondary research. We normalize the number of papers using empirical mehtods for secondary research based on the number of all papers that use secondary research for data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers_per_year = pd.DataFrame(df_query_14.drop_duplicates(subset=['paper']).reset_index(drop=True)['year'].value_counts())\n",
    "all_papers_per_year.columns = ['number_of_papers']\n",
    "all_papers_per_year = all_papers_per_year.sort_index()\n",
    "\n",
    "result = pd.DataFrame(df_query_14.groupby('year')['method_label'].value_counts().unstack())\n",
    "result = result[result.sum().sort_values(ascending=False).index]\n",
    "#display(result)\n",
    "\n",
    "result = pd.concat([result, all_papers_per_year], axis=1)\n",
    "result['normalized archive analysis'] = (result['archive analysis'] / result['number_of_papers']).round(2)\n",
    "result['normalized systematic literature review'] = (result['systematic literature review'] / result['number_of_papers']).round(2)\n",
    "result['normalized literature review'] = (result['literature review'] / result['number_of_papers']).round(2)\n",
    "result['normalized systematic literature map'] = (result['systematic literature map'] / result['number_of_papers']).round(2)\n",
    "result['normalized systematic review'] = (result['systematic review'] / result['number_of_papers']).round(2)\n",
    "result['normalized tertiary literature review'] = (result['tertiary literature review'] / result['number_of_papers']).round(2)\n",
    "#display(result)\n",
    "\n",
    "ax = result.loc[:, 'normalized archive analysis':'normalized tertiary literature review'].plot(kind='bar', rot=0)\n",
    "ax.legend(bbox_to_anchor=(0.4, 0.65), labels=['Archive analysis', 'Systematic literature review', 'Literature review', 'Systematic literature map', 'Systematic review', 'Tertiary literature review'])\n",
    "# make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Normalized number of empirical methods used for secondary research per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Proportion of empirical method used')\n",
    "plt.savefig('Figures/CQ14/proportion_of_empirical_methods_used_for_secondaty_research.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "figure, axis = plt.subplots(3, 2, figsize=(18, 15), sharey=True)\n",
    "# Plot barplots\n",
    "bar1 = sns.barplot(data = result, x = result.index, y='normalized archive analysis', ax=axis[0,0], color=sns.color_palette()[0])\n",
    "bar1.bar_label(bar1.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar1)\n",
    "bar2 = sns.barplot(data = result, x = result.index, y='normalized systematic literature review', ax=axis[0,1], color=sns.color_palette()[1])\n",
    "bar2.bar_label(bar2.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar2)\n",
    "bar3 = sns.barplot(data = result, x = result.index, y='normalized literature review', ax=axis[1,0], color=sns.color_palette()[2])\n",
    "bar3.bar_label(bar3.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar3)\n",
    "bar4 = sns.barplot(data = result, x = result.index, y='normalized systematic literature map', ax=axis[1,1], color=sns.color_palette()[4])\n",
    "bar4.bar_label(bar4.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar4)\n",
    "bar5 = sns.barplot(data = result, x = result.index, y='normalized systematic review', ax=axis[2,0], color=sns.color_palette()[5])\n",
    "bar5.bar_label(bar5.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar5)\n",
    "bar6 = sns.barplot(data = result, x = result.index, y='normalized tertiary literature review', ax=axis[2,1], color=sns.color_palette()[5])\n",
    "bar6.bar_label(bar6.containers[0])\n",
    "# make_axes_area_auto_adjustable(bar6)\n",
    "\n",
    "# # Set titles\n",
    "plt.suptitle(\"Normalized number of empirical methods used for secondary research per year grouped by empirical method\")\n",
    "bar1.set(title = 'Normalized number of archive analysis used for secondary research per year', ylabel='Proportion of archive analysis', xlabel='Year')\n",
    "bar2.set(title = 'Normalized number of systematic literature review used for secondary research per year', ylabel='Proportion of systematic literature review', xlabel='Year')\n",
    "bar3.set(title = 'Normalized number of literature review used for  secondary research per year', ylabel='Proportion of literature review', xlabel='Year')\n",
    "bar4.set(title = 'Normalized number of systematic literature map used for  secondary research per year', ylabel='Proportion of systematic literature map', xlabel='Year')\n",
    "bar5.set(title = 'Normalized number of systematic review used for  secondary research per year', ylabel='Proportion of systematic review', xlabel='Year')\n",
    "bar6.set(title = 'Normalized number of tertiary literature review used for  secondary research per year', ylabel='Proportion of tertiary literature review', xlabel='Year')\n",
    "\n",
    "# # Set spacing between subplots\n",
    "figure.tight_layout()\n",
    "plt.savefig('Figures/CQ14/proportion_of_empirical_methods_used_for_secondary_research_grouped_by_method.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Interpretation*</ins>\n",
    "Muss noch geschrieben werden, wenn die Daten alle geprüft sind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q15'></a>\n",
    "### 3.15 How many different research methods are used per publication?\n",
    "\n",
    "#### <ins>*Data Selection*</ins> \n",
    "\n",
    "*Explanation of the Competency Question*:\n",
    "\n",
    "According to [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30), the **\"current\" state of practice (2007)** shows that there is **limited** advice on how to **combine data from diverse study types**. For the **target state (2020 - 2025)**, [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30) envision that **methods are available** for synthesizing evidence from **variety of perspectives and approaches**. This predicted change leads to the corresponding competency question.\n",
    "\n",
    "*Required Data for the Analysis*:\n",
    "\n",
    "We must retrieve all papers that use our ORKG template and report on the use of empirical methods. According to [Dan (2017)](https://doi.org/10.1002/9781118901731.iecrm0083), empirical methods include data collection and data analysis. An empirical method can therefore be a method for data collection and data analysis. For this reason, we identify number of empirical methods used for data collection and data analysis respectively. Based on the individual paper, we can determine the sum of all empirical methods per publication.\n",
    "\n",
    "[Back to top](#step3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Collection*</ins> \n",
    "Below, we retrieve the required data from the developed body of knowledge of empirical research in RE from the [ORKG](https://www.orkg.org/orkg/) using its [SPARQL endpoint](https://orkg.org/sparql/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = '15'\n",
    "\n",
    "query_15_1 = \"\"\"\n",
    "        SELECT ?paper, (COUNT(?dc_label) AS ?number_of_dc_methods), ?year\n",
    "        WHERE {\n",
    "                ?paper orkgp:P31 ?contribution;\n",
    "                        orkgp:P29 ?year.\n",
    "                ?contribution a orkgc:C27001.\n",
    "\n",
    "                ?contribution orkgp:P56008 ?data_collection.\n",
    "                ?data_collection rdfs:label ?dc_label.\n",
    "                \n",
    "                FILTER((?dc_label != \"no collection\"^^xsd:string))\n",
    "        } GROUP BY ?paper ?year\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_15_1)\n",
    "df_query_15_1 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')\n",
    "\n",
    "query_15_2 = \"\"\"\n",
    "        SELECT ?paper, (COUNT(DISTINCT ?inferential) AS ?number_of_inf_methods), (COUNT(DISTINCT ?descriptive) AS ?number_of_des_methods), (COUNT(DISTINCT ?machine_learning) AS ?number_of_ml_methods), (COUNT(DISTINCT ?other_methods) AS ?number_of_other_methods), ?year\n",
    "        WHERE {\n",
    "                ?paper orkgp:P31 ?contribution;\n",
    "                        orkgp:P29 ?year.\n",
    "                ?contribution a orkgc:C27001.\n",
    "\n",
    "                ?contribution orkgp:P15124 ?data_analysis.\n",
    "                ?data_analysis rdfs:label ?da_label.\n",
    "                \n",
    "                OPTIONAL{?data_analysis orkgp:P56043 ?inferential.}\n",
    "                OPTIONAL{?data_analysis orkgp:P56048 ?descriptive.}\n",
    "                OPTIONAL{?data_analysis orkgp:P57016 ?machine_learning.}\n",
    "                OPTIONAL{?data_analysis orkgp:P76003 ?other_methods.}\n",
    "                \n",
    "                FILTER(?da_label != \"no analysis\"^^xsd:string)\n",
    "                } GROUP BY ?paper ?year\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_15_2)\n",
    "df_query_15_2 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Exploration*</ins>\n",
    "Below, we explore the retrieved data, including its cleaning and validation, to prepare the data for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_15_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_15_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Analysis*</ins>\n",
    "For this data analysis, we consider all papers that that use our ORKG template and secondardy research as the empirical method for data collection. From these papers, we extract the empirical methods used for secondary research. We normalize the number of papers using empirical mehtods for secondary research based on the number of all papers that use secondary research for data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(df_query_15_1, df_query_15_2, on=['paper', 'year'], how='outer').fillna(0)\n",
    "result['number_of_all_methods'] = result['number_of_dc_methods'] + result.loc[:,'number_of_inf_methods':'number_of_other_methods'].sum(axis=1)\n",
    "#display(result)\n",
    "\n",
    "number_of_papers_with_x_methods = pd.DataFrame(result.groupby('number_of_all_methods')['paper'].count())\n",
    "number_of_papers_with_x_methods['normalized'] = (number_of_papers_with_x_methods['paper'] / number_of_papers_with_x_methods['paper'].sum()).round(3)\n",
    "#display(number_of_papers_with_x_methods)\n",
    "\n",
    "plt.figure()\n",
    "ax = number_of_papers_with_x_methods['paper'].plot(kind='bar', rot=0)\n",
    "ax.bar_label(ax.containers[0])\n",
    "#make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Number of papers using X empirical methods for data collection and data analysis')\n",
    "plt.xlabel('Number of empirical methods used')\n",
    "plt.ylabel('Number of papers using X empirical method')\n",
    "plt.savefig('Figures/CQ15/number_of_papers_using_x_empirical_methods.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "ax = number_of_papers_with_x_methods['normalized'].plot(kind='bar', rot=0)\n",
    "ax.bar_label(ax.containers[0])\n",
    "#make_axes_area_auto_adjustable(ax)\n",
    "plt.title('Normalized number of papers using X empirical methods for data collection and data analysis')\n",
    "plt.xlabel('Number of empirical methods used')\n",
    "plt.ylabel('Proportion of papers using X empirical method')\n",
    "plt.savefig('Figures/CQ15/proportion_of_papers_using_x_empirical_methods.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Interpretation*</ins>\n",
    "Muss noch geschrieben werden, wenn die Daten alle geprüft sind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q16'></a>\n",
    "### 3.16 How has the number of research methods used per publication evolved over time?\n",
    "\n",
    "#### <ins>*Data Selection*</ins> \n",
    "\n",
    "*Explanation of the Competency Question*:\n",
    "\n",
    "According to [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30), the **\"current\" state of practice (2007)** shows that there is **limited** advice on how to **combine data from diverse study types**. For the **target state (2020 - 2025)**, [Sjøberg et al. (2007)](https://doi.org/10.1109/FOSE.2007.30) envision that **methods are available** for synthesizing evidence from **variety of perspectives and approaches**. This predicted change leads to the corresponding competency question.\n",
    "\n",
    "*Required Data for the Analysis*:\n",
    "\n",
    "We must retrieve all papers that use our ORKG template and report on the use of empirical methods. According to [Dan (2017)](https://doi.org/10.1002/9781118901731.iecrm0083), empirical methods include data collection and data analysis. An empirical method can therefore be a method for data collection and data analysis. For this reason, we identify number of empirical methods used for data collection and data analysis respectively. Based on the individual paper, we can determine the sum of all empirical methods per publication.\n",
    "\n",
    "[Back to top](#step3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Collection*</ins> \n",
    "Below, we retrieve the required data from the developed body of knowledge of empirical research in RE from the [ORKG](https://www.orkg.org/orkg/) using its [SPARQL endpoint](https://orkg.org/sparql/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = '16'\n",
    "\n",
    "query_16_1 = \"\"\"\n",
    "        SELECT ?paper, (COUNT(?dc_label) AS ?number_of_dc_methods), ?year\n",
    "        WHERE {\n",
    "                ?paper orkgp:P31 ?contribution;\n",
    "                        orkgp:P29 ?year.\n",
    "                ?contribution a orkgc:C27001.\n",
    "\n",
    "                ?contribution orkgp:P56008 ?data_collection.\n",
    "                ?data_collection rdfs:label ?dc_label.\n",
    "                \n",
    "                FILTER((?dc_label != \"no collection\"^^xsd:string))\n",
    "        } GROUP BY ?paper ?year\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_16_1)\n",
    "df_query_16_1 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')\n",
    "\n",
    "query_16_2 = \"\"\"\n",
    "        SELECT ?paper, (COUNT(DISTINCT ?inferential) AS ?number_of_inf_methods), (COUNT(DISTINCT ?descriptive) AS ?number_of_des_methods), (COUNT(DISTINCT ?machine_learning) AS ?number_of_ml_methods), (COUNT(DISTINCT ?other_methods) AS ?number_of_other_methods), ?year\n",
    "        WHERE {\n",
    "                ?paper orkgp:P31 ?contribution;\n",
    "                        orkgp:P29 ?year.\n",
    "                ?contribution a orkgc:C27001.\n",
    "\n",
    "                ?contribution orkgp:P15124 ?data_analysis.\n",
    "                ?data_analysis rdfs:label ?da_label.\n",
    "                \n",
    "                OPTIONAL{?data_analysis orkgp:P56043 ?inferential.}\n",
    "                OPTIONAL{?data_analysis orkgp:P56048 ?descriptive.}\n",
    "                OPTIONAL{?data_analysis orkgp:P57016 ?machine_learning.}\n",
    "                OPTIONAL{?data_analysis orkgp:P76003 ?other_methods.}\n",
    "                \n",
    "                FILTER(?da_label != \"no analysis\"^^xsd:string)\n",
    "                } GROUP BY ?paper ?year\n",
    "        \"\"\"\n",
    "\n",
    "retrieve_data(ID, query_16_2)\n",
    "df_query_16_2 = pd.read_csv('Data/query_'+ ID + '_data_' + DATE + '.csv', encoding='utf-8', encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Exploration*</ins>\n",
    "Below, we explore the retrieved data, including its cleaning and validation, to prepare the data for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_16_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_query_16_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Analysis*</ins>\n",
    "For this data analysis, we consider all papers that that use our ORKG template and secondardy research as the empirical method for data collection. From these papers, we extract the empirical methods used for secondary research. We normalize the number of papers using empirical mehtods for secondary research based on the number of all papers that use secondary research for data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(df_query_16_1, df_query_16_2, on=['paper', 'year'], how='outer').fillna(0)\n",
    "result['number_of_all_methods'] = result['number_of_dc_methods'] + result.loc[:,'number_of_inf_methods':'number_of_other_methods'].sum(axis=1)\n",
    "papers_per_year = result.groupby(['year', 'number_of_all_methods'])['paper'].count()\n",
    "#display(papers_per_year)\n",
    "\n",
    "data = papers_per_year.unstack(level=1, fill_value=0)\n",
    "display(data)\n",
    "\n",
    "ax = data.plot(kind='bar', subplots=True, rot=0, figsize=(18, 15), layout=(5, 2),  sharey=True, sharex=False, xlabel='Year', ylabel='Number of papers', legend=None, title='Number of papers using X empirical methods per year grouped by number of empirical methods')\n",
    "ax[0,0].set_title('Number of papers using 0 empricial methods per year')\n",
    "ax[0,1].set_title('Number of papers using 1 empricial method per year')\n",
    "ax[1,0].set_title('Number of papers using 2 empricial methods per year')\n",
    "ax[1,1].set_title('Number of papers using 3 empricial methods per year')\n",
    "ax[2,0].set_title('Number of papers using 4 empricial methods per year')\n",
    "ax[2,1].set_title('Number of papers using 5 empricial methods per year')\n",
    "ax[3,0].set_title('Number of papers using 6 empricial methods per year')\n",
    "ax[3,1].set_title('Number of papers using 7 empricial methods per year')\n",
    "ax[4,0].set_title('Number of papers using 10 empricial methods per year')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Figures/CQ16/number_of_papers_using_x_empirical_methods_per_year.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "number_of_papers_with_x_methods = pd.DataFrame(result.groupby(['year'])['paper'].count())\n",
    "data['normalized 0.0'] = (data.iloc[:,0] / number_of_papers_with_x_methods.iloc[:,0]).round(2)\n",
    "data['normalized 1.0'] = (data.iloc[:,1] / number_of_papers_with_x_methods.iloc[:,0]).round(2)\n",
    "data['normalized 2.0'] = (data.iloc[:,2] / number_of_papers_with_x_methods.iloc[:,0]).round(2)\n",
    "data['normalized 3.0'] = (data.iloc[:,3] / number_of_papers_with_x_methods.iloc[:,0]).round(2)\n",
    "data['normalized 4.0'] = (data.iloc[:,4] / number_of_papers_with_x_methods.iloc[:,0]).round(2)\n",
    "data['normalized 5.0'] = (data.iloc[:,5] / number_of_papers_with_x_methods.iloc[:,0]).round(2)\n",
    "data['normalized 6.0'] = (data.iloc[:,6] / number_of_papers_with_x_methods.iloc[:,0]).round(2)\n",
    "data['normalized 7.0'] = (data.iloc[:,7] / number_of_papers_with_x_methods.iloc[:,0]).round(2)\n",
    "data['normalized 10.0'] = (data.iloc[:,8] / number_of_papers_with_x_methods.iloc[:,0]).round(2)\n",
    "\n",
    "#display(data.iloc[:,9:18])\n",
    "\n",
    "ax = data.iloc[:,9:18].plot(kind='bar', subplots=True, rot=0, figsize=(18, 15), layout=(5, 2), sharey=True, sharex=False, xlabel='Year', ylabel='Number of papers', legend=None, title='Normalized number of papers using X empirical methods per year grouped by number of empirical methods')\n",
    "ax[0,0].set_title('Normalized number of papers using 0 empricial methods per year')\n",
    "ax[0,1].set_title('Normalized number of papers using 1 empricial method per year')\n",
    "ax[1,0].set_title('Normalized number of papers using 2 empricial methods per year')\n",
    "ax[1,1].set_title('Normalized number of papers using 3 empricial methods per year')\n",
    "ax[2,0].set_title('Normalized number of papers using 4 empricial methods per year')\n",
    "ax[2,1].set_title('Normalized number of papers using 5 empricial methods per year')\n",
    "ax[3,0].set_title('Normalized number of papers using 6 empricial methods per year')\n",
    "ax[3,1].set_title('Normalized number of papers using 7 empricial methods per year')\n",
    "ax[4,0].set_title('Normalized number of papers using 10 empricial methods per year')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Figures/CQ16/proportion_of_papers_using_x_empirical_methods_per_year.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>*Data Interpretation*</ins>\n",
    "Muss noch geschrieben werden, wenn die Daten alle geprüft sind."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16a8259e1791bb0fb9bae0f359302269cb6b9f3eeeb34666ca6d1d1ecd9183a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
